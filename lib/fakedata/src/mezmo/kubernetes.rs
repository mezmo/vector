use crate::mezmo::choose_weighted;
use chrono::Utc;
use faker_rand::{en_us::internet::Domain, lorem::Word};
use rand::{Rng, thread_rng};
use serde::Serialize;

const LEVELS: [(&str, f32); 2] = [("INFO", 15.0), ("ERROR", 1.0)];
const APPS: [(&str, f32); 3] = [
    ("user-service", 8.0),
    ("notification-service", 3.0),
    ("analytics-engine", 0.5),
];
const CONTAINERS: [(&str, f32); 3] = [
    ("liveness-probe", 8.0),
    ("sysdig-runtime-scanner", 2.5),
    ("kafka-broker", 1.5),
];
const INFO_MESSAGES: [(&str, f32); 10] = [
    (
        "Successfully created the pod 'user-service-abcde' with 2 replicas. ... Details: { Kind: Pod, APIVersion: v1, Name: user-service-abcde, Namespace: default, UID: uuid-k8s-111, CreationTimestamp: 2025-04-01T04:45:01Z, Owner: ReplicaSet/user-service-rs, Node: node-01, Phase: Running, Status: { Conditions: [{type: Ready, status: True}], ContainerStatuses: [{name: main, ready: true, restartCount: 0, image: myrepo/user-service:1.1}, {name: sidecar, ready: true, restartCount: 0, image: proxy:1.19}] }, Spec: { Containers: [...], Volumes: [...] } } ... [Simulated Detail Repeated For Length] Event{type=Normal, reason=Scheduled, message='Assigned default/user-service-abcde to node-01'} Event{type=Normal, reason=Pulling, message='Pulling image main'} Event{type=Normal, reason=Pulled, message='Pulled image main'} Event{type=Normal, reason=Created, message='Created container main'} Event{type=Normal, reason=Started, message='Started container main'} Event{type=Normal, reason=Pulling, message='Pulling image sidecar'} Event{type=Normal, reason=Pulled, message='Pulled image sidecar'} Event{type=Normal, reason=Created, message='Created container sidecar'} Event{type=Normal, reason=Started, message='Started container sidecar'} Event{type=Normal, reason=Scheduled, message='Assigned default/user-service-abcde to node-01'} Event{type=Normal, reason=Pulling, message='Pulling image main'} Event{type=Normal, reason=Pulled, message='Pulled image main'} Event{type=Normal, reason=Created, message='Created container main'} Event{type=Normal, reason=Started, message='Started container main'} Event{type=Normal, reason=Pulling, message='Pulling image sidecar'} Event{type=Normal, reason=Pulled, message='Pulled image sidecar'} Event{type=Normal, reason=Created, message='Created container sidecar'} Event{type=Normal, reason=Started, message='Started container sidecar'} Event{type=Normal, reason=Scheduled, message='Assigned default/user-service-abcde to node-01'} Event{type=Normal, reason=Pulling, message='Pulling image main'} Event{type=Normal, reason=Pulled, message='Pulled image main'} Event{type=Normal, reason=Created, message='Created container main'} Event{type=Normal, reason=Started, message='Started container main'} Event{type=Normal, reason=Pulling, message='Pulling image sidecar'} Event{type=Normal, reason=Pulled, message='Pulled image sidecar'} Event{type=Normal, reason=Created, message='Created container sidecar'} Event{type=Normal, reason=Started, message='Started container sidecar'} Event{type=Normal, reason=Scheduled, message='Assigned default/user-service-abcde to node-01'} Event{type=Normal, reason=Pulling, message='Pulling image main'} Event{type=Normal, reason=Pulled, message='Pulled image main'} Event{type=Normal, reason=Created, message='Created container main'} Event{type=Normal, reason=Started, message='Started container main'} Event{type=Normal, reason=Pulling, message='Pulling image sidecar'} Event{type=Normal, reason=Pulled, message='Pulled image sidecar'} Event{type=Normal, reason=Created, message='Created container sidecar'} Event{type=Normal, reason=Started, message='Started container sidecar'} END_DETAIL",
        16.0,
    ),
    (
        "Deployment 'frontend' scaled down to 3 replicas. ... Details: { Kind: Deployment, APIVersion: apps/v1, Name: frontend, Namespace: default, DesiredReplicas: 3, CurrentReplicas: 4, UpdatedReplicas: 3, AvailableReplicas: 3, ReadyReplicas: 3, Reason: ManualScale, TriggeredBy: 'kubectl scale deployment frontend --replicas=3', Timestamp: 2025-04-01T04:45:02Z, OldReplicaSet: frontend-rs-old, NewReplicaSet: frontend-rs-new, TerminatedPods: [frontend-rs-old-pod4] } ... [Simulated Detail Repeated For Length] ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} ScalingEvent{resource=Deployment/frontend, action=ScaleDown, from=4, to=3, time=...} END_DETAIL",
        10.0,
    ),
    (
        "Node 'node-05' became ready and is now available for scheduling. ... Details: { NodeName: 'node-05', Event: 'NodeReady', Timestamp: '2025-04-01T04:45:03Z', Reason: 'KubeletReady', Message: 'kubelet is posting ready status', PreviousStatus: NotReady, LastTransitionTime: '2025-04-01T04:45:03Z', Conditions: [{Type: Ready, Status: True}, {Type: MemoryPressure, Status: False}, {Type: DiskPressure, Status: False}], Capacity: {cpu: '16', memory: '64Gi'}, Allocatable: {cpu: '15.5', memory: '60Gi'}, NodeInfo: { KubeletVersion: 'v1.28.5', OSImage: 'Ubuntu 22.04 LTS', KernelVersion: '5.15.0-custom' } } ... [Simulated Detail Repeated For Length] NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} NodeConditionUpdate{node=node-05, condition=Ready, status=True, time=...} END_DETAIL",
        6.0,
    ),
    (
        "Service 'backend-api' is now accessible at endpoint 'http://backend-api:8080'. ... Details: { Kind: Endpoints, APIVersion: v1, Name: backend-api, Namespace: production, Subsets: [{ Addresses: [{ip: 10.42.2.10, nodeName: node-02, targetRef: {kind: Pod, name: backend-api-pod-1}}, {ip: 10.42.3.11, nodeName: node-03, targetRef: {kind: Pod, name: backend-api-pod-2}}], Ports: [{name: http, port: 8080, protocol: TCP}] }], ResourceVersion: '1234100', CreationTimestamp: '2025-04-01T04:45:04Z', ServiceDetails: { ClusterIP: '10.0.1.150', Type: ClusterIP } } ... [Simulated Detail Repeated For Length] EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} EndpointReady{ip=10.42.3.11, port=8080, ready=true} EndpointReady{ip=10.42.2.10, port=8080, ready=true} END_DETAIL",
        2.0,
    ),
    (
        "ConfigMap 'app-config' updated with new settings in namespace 'staging'. ... Details: { Kind: ConfigMap, APIVersion: v1, Name: app-config, Namespace: staging, UID: uuid-cm-222, ResourceVersion: '1235001', CreationTimestamp: '2025-03-30T10:00:00Z', ChangeTimestamp: '2025-04-01T04:45:05Z', Data: { DB_HOST: 'db.staging.internal', DB_PORT: '5432', FEATURE_X_ENABLED: 'true', LOG_LEVEL: 'INFO', CACHE_SIZE_MB: '512' }, BinaryData: null, Immutable: false, ManagedFields: [...] } ... [Simulated Detail Repeated For Length] ConfigChange{key=LOG_LEVEL, old=DEBUG, new=INFO} ConfigChange{key=CACHE_SIZE_MB, old=256, new=512} ConfigChange{key=FEATURE_X_ENABLED, old=null, new=true} ConfigChange{key=LOG_LEVEL, old=DEBUG, new=INFO} ConfigChange{key=CACHE_SIZE_MB, old=256, new=512} ConfigChange{key=FEATURE_X_ENABLED, old=null, new=true} ConfigChange{key=LOG_LEVEL, old=DEBUG, new=INFO} ConfigChange{key=CACHE_SIZE_MB, old=256, new=512} ConfigChange{key=FEATURE_X_ENABLED, old=null, new=true} ConfigChange{key=LOG_LEVEL, old=DEBUG, new=INFO} ConfigChange{key=CACHE_SIZE_MB, old=256, new=512} ConfigChange{key=FEATURE_X_ENABLED, old=null, new=true} ConfigChange{key=LOG_LEVEL, old=DEBUG, new=INFO} ConfigChange{key=CACHE_SIZE_MB, old=256, new=512} ConfigChange{key=FEATURE_X_ENABLED, old=null, new=true} ConfigChange{key=LOG_LEVEL, old=DEBUG, new=INFO} ConfigChange{key=CACHE_SIZE_MB, old=256, new=512} ConfigChange{key=FEATURE_X_ENABLED, old=null, new=true} ConfigChange{key=LOG_LEVEL, old=DEBUG, new=INFO} ConfigChange{key=CACHE_SIZE_MB, old=256, new=512} ConfigChange{key=FEATURE_X_ENABLED, old=null, new=true} ConfigChange{key=LOG_LEVEL, old=DEBUG, new=INFO} ConfigChange{key=CACHE_SIZE_MB, old=256, new=512} ConfigChange{key=FEATURE_X_ENABLED, old=null, new=true} ConfigChange{key=LOG_LEVEL, old=DEBUG, new=INFO} ConfigChange{key=CACHE_SIZE_MB, old=256, new=512} ConfigChange{key=FEATURE_X_ENABLED, old=null, new=true} ConfigChange{key=LOG_LEVEL, old=DEBUG, new=INFO} ConfigChange{key=CACHE_SIZE_MB, old=256, new=512} ConfigChange{key=FEATURE_X_ENABLED, old=null, new=true} ConfigChange{key=LOG_LEVEL, old=DEBUG, new=INFO} ConfigChange{key=CACHE_SIZE_MB, old=256, new=512} ConfigChange{key=FEATURE_X_ENABLED, old=null, new=true} ConfigChange{key=LOG_LEVEL, old=DEBUG, new=INFO} ConfigChange{key=CACHE_SIZE_MB, old=256, new=512} ConfigChange{key=FEATURE_X_ENABLED, old=null, new=true} ConfigChange{key=LOG_LEVEL, old=DEBUG, new=INFO} ConfigChange{key=CACHE_SIZE_MB, old=256, new=512} ConfigChange{key=FEATURE_X_ENABLED, old=null, new=true} ConfigChange{key=LOG_LEVEL, old=DEBUG, new=INFO} ConfigChange{key=CACHE_SIZE_MB, old=256, new=512} ConfigChange{key=FEATURE_X_ENABLED, old=null, new=true} ConfigChange{key=LOG_LEVEL, old=DEBUG, new=INFO} ConfigChange{key=CACHE_SIZE_MB, old=256, new=512} ConfigChange{key=FEATURE_X_ENABLED, old=null, new=true} ConfigChange{key=LOG_LEVEL, old=DEBUG, new=INFO} ConfigChange{key=CACHE_SIZE_MB, old=256, new=512} ConfigChange{key=FEATURE_X_ENABLED, old=null, new=true} ConfigChange{key=LOG_LEVEL, old=DEBUG, new=INFO} ConfigChange{key=CACHE_SIZE_MB, old=256, new=512} ConfigChange{key=FEATURE_X_ENABLED, old=null, new=true} END_DETAIL",
        1.0,
    ),
    (
        "Pod 'order-service-pod-xyz789' started successfully and is now running in namespace 'production'. ... Details: { Kind: Pod, Name: order-service-pod-xyz789, Namespace: production, Phase: Running, StartTime: 2025-04-01T04:45:06Z, Node: node-03, IP: 10.42.3.15, Controller: ReplicaSet/order-service-rs, Containers: [{Name: order-app, Image: myrepo/order:3.1, State: Running, Ready: True, Restarts: 0, Ports: [8000/TCP], Resources: {Requests: {cpu: 500m, memory: 1Gi}, Limits: {cpu: 1, memory: 2Gi}}}] } ... [Simulated Detail Repeated For Length] ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} ContainerState{name=order-app, state=RUNNING, ready=true, startedAt=...} END_DETAIL",
        1.0,
    ),
    (
        "PersistentVolumeClaim 'data-claim' successfully bound to 'data-volume' in namespace 'data-services'. ... Details: { Kind: PersistentVolumeClaim, Name: data-claim, Namespace: data-services, Phase: Bound, VolumeName: data-volume, StorageClass: fast-ssd, AccessModes: [ReadWriteOnce], Capacity: 100Gi, VolumeMode: Filesystem, UID: uuid-pvc-333, ResourceVersion: '1235500', CreationTimestamp: '2025-04-01T04:40:00Z', BindTimestamp: '2025-04-01T04:45:07Z', BoundByController: pv-controller, SelectedNode: node-06, PVInfo: { Name: data-volume, UID: uuid-pv-999, StorageClass: fast-ssd, Capacity: 100Gi, Source: AWSElasticBlockStore(vol-abc...) } } ... [Simulated Detail Repeated For Length] VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} VolumeStatus{pv=data-volume, phase=Bound, claim=data-services/data-claim, reclaimPolicy=Retain} END_DETAIL",
        0.5,
    ),
    (
        "Ingress 'web-ingress' has been updated with new routing rules in namespace 'default'. ... Details: { Kind: Ingress, Name: web-ingress, Namespace: default, UID: uuid-ing-444, ResourceVersion: '1236005', UpdateTimestamp: '2025-04-01T04:45:08Z', ClassName: 'nginx', Rules: [{Host: app.example.com, Http: {Paths: [{Path: /api, PathType: Prefix, Backend: {Service: {Name: backend-api, Port: {Number: 80}}}}, {Path: /, PathType: Prefix, Backend: {Service: {Name: frontend, Port: {Number: 80}}}}]}}], TLS: [{Hosts: [app.example.com], SecretName: app-tls-secret}], Annotations: {'nginx.ingress.kubernetes.io/rewrite-target': '/', 'cert-manager.io/cluster-issuer': 'letsencrypt-prod', 'nginx.ingress.kubernetes.io/proxy-body-size': '50m'}, Status: {LoadBalancer: {Ingress: [{IP: 192.0.2.100}]}} } ... [Simulated Detail Repeated For Length] IngressRuleUpdate{host=app.example.com, path=/api, backend=backend-api:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/, backend=frontend:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/api, backend=backend-api:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/, backend=frontend:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/api, backend=backend-api:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/, backend=frontend:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/api, backend=backend-api:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/, backend=frontend:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/api, backend=backend-api:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/, backend=frontend:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/api, backend=backend-api:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/, backend=frontend:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/api, backend=backend-api:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/, backend=frontend:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/api, backend=backend-api:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/, backend=frontend:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/api, backend=backend-api:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/, backend=frontend:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/api, backend=backend-api:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/, backend=frontend:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/api, backend=backend-api:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/, backend=frontend:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/api, backend=backend-api:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/, backend=frontend:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/api, backend=backend-api:80, action=UPDATE} IngressRuleUpdate{host=app.example.com, path=/, backend=frontend:80, action=UPDATE} END_DETAIL",
        1.0,
    ),
    (
        "Pod 'task-processor-job-1a2b3c-ghjkl' completed execution and is terminating in namespace 'batch-jobs'. ... Details: { Kind: Pod, Name: task-processor-job-1a2b3c-ghjkl, Namespace: batch-jobs, Phase: Succeeded, Reason: Completed, Message: 'Job completed successfully', StartTime: '2025-04-01T04:40:00Z', FinishTime: '2025-04-01T04:45:09Z', DurationSec: 309, ContainerStatuses: [{Name: processor, State: {Terminated: {ExitCode: 0, Reason: Completed, StartedAt: '...', FinishedAt: '...'}}, Ready: false, RestartCount: 0, Image: myrepo/processor:batch-v1}], Owner: Job/task-processor-job-1a2b3c } ... [Simulated Detail Repeated For Length] JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} JobCompletion{job=task-processor-job-1a2b3c, status=SUCCESS, duration=309s} END_DETAIL",
        1.0,
    ),
    (
        "Service 'frontend' has been assigned the ClusterIP '10.0.0.10' in namespace 'default'. ... Details: { Kind: Service, Name: frontend, Namespace: default, ClusterIP: '10.0.0.10', IPFamilies: [IPv4], IPFamilyPolicy: SingleStack, Ports: [{Name: http, Port: 80, Protocol: TCP, TargetPort: 8080}], Type: ClusterIP, SessionAffinity: None, Selector: {app: frontend, tier: web}, CreationTimestamp: '2025-03-31T13:55:00Z', ResourceVersion: '1230010' } ... [Simulated Detail Repeated For Length] ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} ServiceEndpoint{ip='10.0.0.10', port=80, protocol=TCP} END_DETAIL",
        1.0,
    ),
];

const ERROR_MESSAGES: [(&str, f32); 10] = [
    (
        "Failed to create the pod 'user-service-failed-pod' due to resource limits. ... Details: { Kind: Pod, Name: user-service-failed-pod, Namespace: production, Phase: Failed, Reason: FailedScheduling, Message: '0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had insufficient cpu.', PodUID: uuid-k8s-err-111, CreationTimestamp: '2025-04-01T04:45:11Z', Controller: ReplicaSet/user-service-rs, Node: null, RequestedResources: {cpu: 2000m, memory: 4Gi}, NodeStatusSummary: { node-01: { Taints: [], AffinityMatch: true, CPUAvailable: 1500m (INSUFFICIENT) }, node-02: { Taints: [], AffinityMatch: true, CPUAvailable: 500m (INSUFFICIENT) }, node-03: { Taints: [], AffinityMatch: true, CPUAvailable: 1000m (INSUFFICIENT) }, node-master-1: { Taints: [master], AffinityMatch: false }, node-master-2: { Taints: [master], AffinityMatch: false } } } ... [Simulated Detail Repeated For Length] SchedulingFailure{reason=InsufficientCPU, count=3} SchedulingFailure{reason=NodeAffinityMismatch, count=3} SchedulingFailure{reason=UntoleratedTaint, count=2} SchedulingFailure{reason=InsufficientCPU, count=3} SchedulingFailure{reason=NodeAffinityMismatch, count=3} SchedulingFailure{reason=UntoleratedTaint, count=2} SchedulingFailure{reason=InsufficientCPU, count=3} SchedulingFailure{reason=NodeAffinityMismatch, count=3} SchedulingFailure{reason=UntoleratedTaint, count=2} SchedulingFailure{reason=InsufficientCPU, count=3} SchedulingFailure{reason=NodeAffinityMismatch, count=3} SchedulingFailure{reason=UntoleratedTaint, count=2} SchedulingFailure{reason=InsufficientCPU, count=3} SchedulingFailure{reason=NodeAffinityMismatch, count=3} SchedulingFailure{reason=UntoleratedTaint, count=2} SchedulingFailure{reason=InsufficientCPU, count=3} SchedulingFailure{reason=NodeAffinityMismatch, count=3} SchedulingFailure{reason=UntoleratedTaint, count=2} SchedulingFailure{reason=InsufficientCPU, count=3} SchedulingFailure{reason=NodeAffinityMismatch, count=3} SchedulingFailure{reason=UntoleratedTaint, count=2} SchedulingFailure{reason=InsufficientCPU, count=3} SchedulingFailure{reason=NodeAffinityMismatch, count=3} SchedulingFailure{reason=UntoleratedTaint, count=2} SchedulingFailure{reason=InsufficientCPU, count=3} SchedulingFailure{reason=NodeAffinityMismatch, count=3} SchedulingFailure{reason=UntoleratedTaint, count=2} SchedulingFailure{reason=InsufficientCPU, count=3} SchedulingFailure{reason=NodeAffinityMismatch, count=3} SchedulingFailure{reason=UntoleratedTaint, count=2} SchedulingFailure{reason=InsufficientCPU, count=3} SchedulingFailure{reason=NodeAffinityMismatch, count=3} SchedulingFailure{reason=UntoleratedTaint, count=2} SchedulingFailure{reason=InsufficientCPU, count=3} SchedulingFailure{reason=NodeAffinityMismatch, count=3} SchedulingFailure{reason=UntoleratedTaint, count=2} SchedulingFailure{reason=InsufficientCPU, count=3} SchedulingFailure{reason=NodeAffinityMismatch, count=3} SchedulingFailure{reason=UntoleratedTaint, count=2} END_DETAIL",
        10.0,
    ),
    (
        "Deployment 'frontend' failed to scale due to insufficient nodes. ... Details: { Kind: Deployment, Name: frontend, Namespace: default, Reason: FailedScale, Message: 'Cannot scale deployment frontend to 5 replicas: 0/5 nodes are available: 5 node(s) had insufficient memory.', DesiredReplicas: 5, CurrentReadyReplicas: 3, Timestamp: 2025-04-01T04:45:12Z, Controller: deployment-controller, Selector: app=frontend, RequiredResourcesPerPod: {memory: 2Gi}, NodeMemoryAvailable: {node-01: 1.5Gi, node-02: 1.8Gi, node-03: 0.5Gi, node-04: 1.9Gi, node-05: 1.2Gi} } ... [Simulated Detail Repeated For Length] ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} ScaleFailure{reason=InsufficientMemory, nodes_checked=5, nodes_failed=5} END_DETAIL",
        7.0,
    ),
    (
        "Node 'node-05' is not responding; marking as unhealthy. ... Details: { Kind: Node, Name: node-05, Event: NodeNotReady, Timestamp: 2025-04-01T04:45:13Z, Reason: KubeletNotReady, Message: 'Node node-05 status is now: NodeNotReady. Kubelet stopped posting node status. Last heartbeat: 2025-04-01T04:40:00Z (5m13s ago)', NodeControllerStatus: NodeStatusUpdateTimedOut, EvictionStatus: Evicting pods starting now. Conditions: [{Type: Ready, Status: False, LastTransitionTime: 2025-04-01T04:45:13Z}] } ... [Simulated Detail Repeated For Length] PodEviction{pod=pod-a, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-b, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-c, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-d, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-e, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-f, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-g, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-h, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-i, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-j, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-k, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-l, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-m, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-n, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-o, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-p, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-q, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-r, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-s, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-t, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-u, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-v, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-w, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-x, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-y, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-z, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-a, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-b, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-c, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-d, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-e, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-f, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-g, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-h, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-i, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-j, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-k, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-l, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-m, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-n, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-o, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-p, node=node-05, reason=NodeNotReady} PodEviction{pod=pod-q, node=node-05, reason=NodeNotReady} END_DETAIL",
        3.0,
    ),
    (
        r#"Error pulling image 'nginx:latest' for pod 'frontend-pod-bbbbb'. ... Details: { Kind: Pod, Name: frontend-pod-bbbbb, Namespace: default, Container: nginx-main, Image: nginx:latest, Event: Failed, Reason: ErrImagePull, Message: 'rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/nginx:latest": failed to resolve reference "docker.io/library/nginx:latest": unexpected status code [manifests latest]: 401 Unauthorized', Node: node-02, Timestamp: 2025-04-01T04:45:14Z, RetryCount: 3 } ... [Simulated Detail Repeated For Length] ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} ImagePullFailure{image='nginx:latest', error='401 Unauthorized'} END_DETAIL"#,
        1.0,
    ),
    (
        r#"PersistentVolumeClaim 'data-claim' could not be bound. ... Details: { Kind: PersistentVolumeClaim, Name: data-claim, Namespace: data-services, Phase: Pending, Reason: BindingFailed, Message: 'No PersistentVolume available with StorageClass "standard-gp2" and access mode(s) ReadWriteOnce found.', StorageClassName: 'standard-gp2', RequestedStorage: 20Gi, AccessModes: [ReadWriteOnce], Timestamp: 2025-04-01T04:45:15Z, Controller: persistentvolume-controller } ... [Simulated Detail Repeated For Length] PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} PVCheck{class=standard-gp2, access=RWO, capacity>=20Gi, result=NotFound} END_DETAIL"#,
        1.0,
    ),
    (
        r#"Pod 'order-service-crashed-pod' crashed with exit code 1. ... Details: { Kind: Pod, Name: order-service-crashed-pod, Namespace: production, Phase: Failed, Reason: Error, Message: 'Container main terminated with exit code 1', StartTime: '2025-04-01T04:45:16Z', ContainerStatuses: [{Name: main, State: {Terminated: {ExitCode: 1, Reason: Error, StartedAt: '...', FinishedAt: '...', ContainerID: '...'}}, Ready: false, RestartCount: 4, LastTerminationState: {...}}], Node: node-04 } ... [Simulated Detail Repeated For Length] ContainerLogTail: \nERROR:root:Failed to connect to database: Connection refused\nTraceback (most recent call last):\n  File "/app/server.py", line 150, in connect_db\n  File "/usr/local/lib/python3.9/site-packages/db_driver.py", line 50, in connect\nConnectionRefusedError: [Errno 111] Connection refused\nContainerLogTail: \nERROR:root:Failed to connect to database: Connection refused\nTraceback (most recent call last):\n  File "/app/server.py", line 150, in connect_db\n  File "/usr/local/lib/python3.9/site-packages/db_driver.py", line 50, in connect\nConnectionRefusedError: [Errno 111] Connection refused\nContainerLogTail: \nERROR:root:Failed to connect to database: Connection refused\nTraceback (most recent call last):\n  File "/app/server.py", line 150, in connect_db\n  File "/usr/local/lib/python3.9/site-packages/db_driver.py", line 50, in connect\nConnectionRefusedError: [Errno 111] Connection refused\nContainerLogTail: \nERROR:root:Failed to connect to database: Connection refused\nTraceback (most recent call last):\n  File "/app/server.py", line 150, in connect_db\n  File "/usr/local/lib/python3.9/site-packages/db_driver.py", line 50, in connect\nConnectionRefusedError: [Errno 111] Connection refused\nContainerLogTail: \nERROR:root:Failed to connect to database: Connection refused\nTraceback (most recent call last):\n  File "/app/server.py", line 150, in connect_db\n  File "/usr/local/lib/python3.9/site-packages/db_driver.py", line 50, in connect\nConnectionRefusedError: [Errno 111] Connection refused\nContainerLogTail: \nERROR:root:Failed to connect to database: Connection refused\nTraceback (most recent call last):\n  File "/app/server.py", line 150, in connect_db\n  File "/usr/local/lib/python3.9/site-packages/db_driver.py", line 50, in connect\nConnectionRefusedError: [Errno 111] Connection refused\nContainerLogTail: \nERROR:root:Failed to connect to database: Connection refused\nTraceback (most recent call last):\n  File "/app/server.py", line 150, in connect_db\n  File "/usr/local/lib/python3.9/site-packages/db_driver.py", line 50, in connect\nConnectionRefusedError: [Errno 111] Connection refused\nContainerLogTail: \nERROR:root:Failed to connect to database: Connection refused\nTraceback (most recent call last):\n  File "/app/server.py", line 150, in connect_db\n  File "/usr/local/lib/python3.9/site-packages/db_driver.py", line 50, in connect\nConnectionRefusedError: [Errno 111] Connection refused\nContainerLogTail: \nERROR:root:Failed to connect to database: Connection refused\nEND_DETAIL"#,
        1.0,
    ),
    (
        r#"Failed to update ConfigMap 'app-config' due to invalid syntax. ... Details: { Kind: ConfigMap, Name: app-config, Namespace: staging, Reason: InvalidData, Message: 'ConfigMap "app-config" is invalid: data.CACHE_TTL: Invalid value: "60m": duration must be integer seconds or parsable duration string (e.g., "1h", "90s")', FieldPath: 'data.CACHE_TTL', InvalidValue: '60m', UpdateTimestamp: '2025-04-01T04:45:17Z', ResourceVersion: '1235001', Controller: configmap-controller } ... [Simulated Detail Repeated For Length] InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} InvalidField{field='data.CACHE_TTL', value='60m', error='Invalid duration format'} END_DETAIL"#,
        0.5,
    ),
    (
        r#"Service 'backend-api' is not reachable from the cluster. ... Details: { Kind: Service, Name: backend-api, Namespace: production, Reason: EndpointNotReady, Message: 'Endpoints for service "backend-api" are not ready: No pods match selector "app=backend-api, tier=service". Endpoints controller sync skipped.', Timestamp: 2025-04-01T04:45:18Z, Selector: {app: backend-api, tier: service}, ClusterIP: 10.0.1.150, Ports: [80/TCP] } ... [Simulated Detail Repeated For Length] EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} EndpointSyncStatus{service=backend-api, status=SKIPPED, reason=NoMatchingPods} END_DETAIL"#,
        0.0, // Note: Weight is 0.0 in the input
    ),
    (
        r#"Error while applying Ingress 'web-ingress' rules. ... Details: { Kind: Ingress, Name: web-ingress, Namespace: default, Reason: BackendServiceNotFound, Message: 'Failed to sync ingress resources: Service "default/non-existent-service" not found for backend rule targeting path "/special"', FaultyRule: {Path: /special, Backend: {Service: {Name: non-existent-service, Port: {Number: 80}}}}, Controller: nginx-ingress-controller, Timestamp: 2025-04-01T04:45:19Z, ResourceVersion: '1239000' } ... [Simulated Detail Repeated For Length] IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} IngressValidationError{rule_index=0, path='/special', service='non-existent-service', reason='SERVICE_NOT_FOUND'} END_DETAIL"#,
        1.0,
    ),
    (
        "Pod 'task-processor-crashed-pod' is in CrashLoopBackOff state. ... Details: { Kind: Pod, Name: task-processor-crashed-pod, Namespace: batch-jobs, Phase: Running, Reason: CrashLoopBackOff, Message: 'back-off 5m0s restarting failed container=processor pod=task-processor-crashed-pod_batch-jobs(uuid-k8s-err-555)', RestartCount: 8, LastTerminationState: { ContainerID: '...', ExitCode: 137, FinishedAt: '2025-04-01T04:45:15Z', Reason: OOMKilled', StartedAt: '...' }, Node: node-08 } ... [Simulated Detail Repeated For Length] PodRestart{reason=OOMKilled, exitCode=137, restarts=1} PodRestart{reason=OOMKilled, exitCode=137, restarts=2} PodRestart{reason=OOMKilled, exitCode=137, restarts=3} PodRestart{reason=OOMKilled, exitCode=137, restarts=4} PodRestart{reason=OOMKilled, exitCode=137, restarts=5} PodRestart{reason=OOMKilled, exitCode=137, restarts=6} PodRestart{reason=OOMKilled, exitCode=137, restarts=7} PodRestart{reason=OOMKilled, exitCode=137, restarts=8} PodRestart{reason=OOMKilled, exitCode=137, restarts=1} PodRestart{reason=OOMKilled, exitCode=137, restarts=2} PodRestart{reason=OOMKilled, exitCode=137, restarts=3} PodRestart{reason=OOMKilled, exitCode=137, restarts=4} PodRestart{reason=OOMKilled, exitCode=137, restarts=5} PodRestart{reason=OOMKilled, exitCode=137, restarts=6} PodRestart{reason=OOMKilled, exitCode=137, restarts=7} PodRestart{reason=OOMKilled, exitCode=137, restarts=8} PodRestart{reason=OOMKilled, exitCode=137, restarts=1} PodRestart{reason=OOMKilled, exitCode=137, restarts=2} PodRestart{reason=OOMKilled, exitCode=137, restarts=3} PodRestart{reason=OOMKilled, exitCode=137, restarts=4} PodRestart{reason=OOMKilled, exitCode=137, restarts=5} PodRestart{reason=OOMKilled, exitCode=137, restarts=6} PodRestart{reason=OOMKilled, exitCode=137, restarts=7} PodRestart{reason=OOMKilled, exitCode=137, restarts=8} PodRestart{reason=OOMKilled, exitCode=137, restarts=1} PodRestart{reason=OOMKilled, exitCode=137, restarts=2} PodRestart{reason=OOMKilled, exitCode=137, restarts=3} PodRestart{reason=OOMKilled, exitCode=137, restarts=4} PodRestart{reason=OOMKilled, exitCode=137, restarts=5} PodRestart{reason=OOMKilled, exitCode=137, restarts=6} PodRestart{reason=OOMKilled, exitCode=137, restarts=7} PodRestart{reason=OOMKilled, exitCode=137, restarts=8} PodRestart{reason=OOMKilled, exitCode=137, restarts=1} PodRestart{reason=OOMKilled, exitCode=137, restarts=2} PodRestart{reason=OOMKilled, exitCode=137, restarts=3} PodRestart{reason=OOMKilled, exitCode=137, restarts=4} PodRestart{reason=OOMKilled, exitCode=137, restarts=5} PodRestart{reason=OOMKilled, exitCode=137, restarts=6} PodRestart{reason=OOMKilled, exitCode=137, restarts=7} PodRestart{reason=OOMKilled, exitCode=137, restarts=8} PodRestart{reason=OOMKilled, exitCode=137, restarts=1} PodRestart{reason=OOMKilled, exitCode=137, restarts=2} PodRestart{reason=OOMKilled, exitCode=137, restarts=3} PodRestart{reason=OOMKilled, exitCode=137, restarts=4} PodRestart{reason=OOMKilled, exitCode=137, restarts=5} PodRestart{reason=OOMKilled, exitCode=137, restarts=6} END_DETAIL",
        1.0,
    ),
];

pub fn kubernetes_log_line() -> KubernetesLog {
    KubernetesLog::new()
}

#[derive(Debug, Serialize)]
pub struct KubernetesLog {
    app: String,
    container: String,
    host: String,
    node: String,
    pod: String,
    namespace: String,
    level: String,
    line: String,
}

impl KubernetesLog {
    fn new() -> Self {
        let app = choose_weighted(&APPS);
        let level = choose_weighted(&LEVELS);
        Self {
            app: app.to_string(),
            container: choose_weighted(&CONTAINERS).to_string(),
            host: thread_rng()
                .sample::<Domain, _>(rand::distributions::Standard)
                .to_string(),
            node: thread_rng()
                .sample::<Word, _>(rand::distributions::Standard)
                .to_string(),
            pod: thread_rng()
                .sample::<Word, _>(rand::distributions::Standard)
                .to_string(),
            namespace: thread_rng()
                .sample::<Word, _>(rand::distributions::Standard)
                .to_string(),
            level: level.to_string(),
            line: kubernetes_message(level, app),
        }
    }
}

fn kubernetes_message(level: &str, app: &str) -> String {
    let msg = match level {
        "ERROR" => choose_weighted(&ERROR_MESSAGES).to_string(),
        _ => choose_weighted(&INFO_MESSAGES).to_string(),
    };
    // message format:
    // <datetime:YYYY-MM-DDTHH:MM:SS.MMMZ> <level> <component>: <message>
    format!(
        "{} {} {}: {}",
        Utc::now().to_rfc3339_opts(chrono::SecondsFormat::Millis, true),
        level,
        app,
        msg
    )
}
