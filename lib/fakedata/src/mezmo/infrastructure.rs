use super::access_log::nginx_access_log_line;
use crate::{logs::syslog_5424_log_line, mezmo::choose_weighted};
use chrono::Utc;
use fakedata_generator::gen_ipv4;
use faker_rand::en_us::internet::Domain;
use rand::{Rng, thread_rng};
use serde::Serialize;

const SPARK_LOG_TIME_FORMAT: &str = "%m/%d/%y %H:%M:%S";
const HDFS_LOG_TIME_FORMAT: &str = "%y%m%d %H%M%S";
const LOG_LEVELS: [(&str, f32); 2] = [("INFO", 8.0), ("WARN", 0.5)];
const LOG_SOURCE_TYPES: [(&str, f32); 4] = [
    ("HDFS", 15.0),
    ("SPARK", 8.0),
    ("NGINX", 0.5),
    ("SYSLOG", 0.5),
];
const APPS: [(&str, f32); 3] = [
    ("task-scheduler", 10.0),
    ("chat-service", 8.0),
    ("file-storage", 1.0),
];

const HDFS_COMPONENTS: [(&str, f32); 3] = [
    ("dfs.DataNodePacketResponder", 10.0),
    ("dfs.FSNamesystem", 6.0),
    ("dfs.DataNodeDataXceiver", 0.5),
];
// Each log line is about 3kb in size to aid with cost estimation workflow
const HDFS_INFO_MESSAGES: [(&str, f32); 9] = [
    (
        "BLOCK NameSystem.addStoredBlock: blockMap updated: 10.251.73.220:50010 is added to blk_7128370237687728475 size 67108864. Node capacity: 95%. Available space: 10.5TB. Replication factor: 3. Nodes involved: [dn1.hadoop.local:50010, dn5.hadoop.local:50010, dn8.hadoop.local:50010]. Block pool ID: BP-12345-10.0.0.1-1600000000. Received status update from datanode 10.251.73.220. | DETAILS: { TxId: 500123, Op: AddBlock, File: /user/data/file_part_1.dat, InodeId: 16387, ParentInodeId: 16386, BlockGenerationStamp: 1001, BlockState: COMMITTED, StoragePolicy: HOT, ErasureCodingPolicy: None, StorageTierInfos: [{StorageType: DISK, Nodes: [dn1, dn5, dn8], Locations: [/data/hdfs/dn/current/BP-12345/current/finalized/subdir0/..., /data/hdfs/dn/current/BP-12345/current/finalized/subdir1/..., /data/hdfs/dn/current/BP-12345/current/finalized/subdir2/...]}], User: hdfs, Group: hadoop, Permissions: 0644, AccessTime: 1678886400000, ModificationTime: 1678886400000, LockState: { WriteLockHolder: NameNodeRpcServer(Handler=5), ReadLockCount: 0 }, PendingOps: [], BlockValidationStatus: OK, ChecksumType: CRC32C, BytesPerChecksum: 512, ReplicationQueuePriority: NORMAL, LowRedundancyBlocks: 3, CorruptBlocks: 0, MissingBlocks: 0, DecommissionedReplicas: 0, ActiveReplicas: 3, StaleReplicas: 0, LastHeartbeatDetails: { dn1: { lastSeen: 1s ago, capacity: 10TB, used: 8TB, remaining: 2TB, xceiverLoad: 5 }, dn5: { lastSeen: 2s ago, capacity: 10TB, used: 7.5TB, remaining: 2.5TB, xceiverLoad: 3 }, dn8: { lastSeen: 1s ago, capacity: 12TB, used: 9TB, remaining: 3TB, xceiverLoad: 4 } }, JournalSyncStatus: OK, EditLogTailTxId: 500123, LastAppliedTxId: 500123, NamenodeHeapUsage: 4.8GB/8GB, GCInfo: { Count: 10, Time: 250ms }, RPCQueueLength: 1, ActiveRPCConnections: 50, FilesystemState: HEALTHY, SafeMode: OFF, TotalBlocks: 10000500, TotalFiles: 5000100 } ... [Simulated Detail Block Repeated For Length - Shortened] BlockInfo{id=blk_7128370237687728475, size=67108864, nodes=[dn1,dn5,dn8]} BlockInfo{id=blk_7128370237687728476, size=67108864, nodes=[dn2,dn4,dn6]} BlockInfo{id=blk_7128370237687728477, size=67108864, nodes=[dn3,dn7,dn9]} BlockInfo{id=blk_7128370237687728478, size=67108864, nodes=[dn1,dn4,dn7]} BlockInfo{id=blk_7128370237687728479, size=67108864, nodes=[dn2,dn5,dn8]} BlockInfo{id=blk_7128370237687728480, size=67108864, nodes=[dn3,dn6,dn9]} BlockInfo{id=blk_7128370237687728481, size=67108864, nodes=[dn1,dn5,dn9]} BlockInfo{id=blk_7128370237687728482, size=67108864, nodes=[dn2,dn6,dn7]} BlockInfo{id=blk_7128370237687728483, size=67108864, nodes=[dn3,dn4,dn8]} BlockInfo{id=blk_7128370237687728484, size=67108864, nodes=[dn1,dn6,dn8]} BlockInfo{id=blk_7128370237687728485, size=67108864, nodes=[dn2,dn4,dn9]} BlockInfo{id=blk_7128370237687728486, size=67108864, nodes=[dn3,dn5,dn7]} BlockInfo{id=blk_7128370237687728487, size=67108864, nodes=[dn1,dn4,dn8]} BlockInfo{id=blk_7128370237687728488, size=67108864, nodes=[dn2,dn5,dn7]} BlockInfo{id=blk_7128370237687728489, size=67108864, nodes=[dn3,dn6,dn8]} BlockInfo{id=blk_7128370237687728490, size=67108864, nodes=[]} END_DETAIL",
        1.0,
    ),
    (
        "Receiving block blk_5792489080791696128 src: /10.251.30.6:33145 dest: /10.251.30.6:50010. Block size: 67108864. Pipeline nodes: [dn2.hadoop.local:50010, dn3.hadoop.local:50010, dn7.hadoop.local:50010]. Client name: DFSClient_NONMAPREDUCE_-12345_1. Hedged reads: disabled. Transfer protocol: Protocol Buffers. Security enabled: Kerberos. User: dr.who. Group: tardis. Target file: /data/events/raw/log_stream_part-00153.log.tmp. ... DETAILS: { TransferId: transfer-987, StartTime: 1678886410000, Stage: DATA_TRANSFER, PacketSize: 65536, WindowSize: 1024, AckedPackets: 500, PendingPackets: 10, LastAckSeqNo: 999, ExpectedSeqNo: 1000, BytesReceived: 32768000, ThroughputEstimate: 45.5MB/s, DownstreamNodes: [{Node: dn3.hadoop.local:50010, Status: OK, LastAckTime: 1ms ago}, {Node: dn7.hadoop.local:50010, Status: OK, LastAckTime: 2ms ago}], UpstreamNode: {Node: /10.251.30.6:33145, Status: ACTIVE}, BufferQueueDepth: 5, MaxBufferQueueDepth: 128, DiskWriteQueueDepth: 2, MaxDiskWriteQueueDepth: 64, SyncState: SYNC_PENDING, ChecksumVerification: ONGOING, ChecksumErrors: 0, BlockFileHandle: { Path: /data/hdfs/dn/current/BP-12345/current/rbw/subdir0/blk_5792489080791696128, FD: 72, Position: 32768000 }, MetaFileHandle: { Path: /data/hdfs/dn/current/BP-12345/current/rbw/subdir0/blk_5792489080791696128_1002.meta, FD: 73, Position: 512 }, ThreadInfo: { Name: DataXceiverThread-12, State: RUNNABLE, CPU_ms: 1500 }, NetworkStats: { ReadBytes: 33MB, WriteBytes: 1MB (acks) }, DiskStats: { WriteBytes: 32MB, WriteOps: 500 }, BlockTokenInfo: { Used: true, Expiry: 1678972800000, User: dr.who }, EncryptionInfo: { Zone: /data/events/raw, Enabled: true, Cipher: AES-256-XTS }, RecoveryInfo: { RecoveryId: 0, SyncBlock: false }, PerformanceCounters: { PacketQueueTimeAvg: 0.5ms, DiskWriteLatencyAvg: 5ms, ChecksumLatencyAvg: 0.1ms } } ... [Simulated Detail Block Repeated For Length - Shortened] NodeStatus{ip=10.251.30.6, state=ACTIVE, load=15} NodeStatus{ip=10.251.30.7, state=ACTIVE, load=20} NodeStatus{ip=10.251.30.8, state=ACTIVE, load=18} NodeStatus{ip=10.251.30.9, state=STALE, load=10} NodeStatus{ip=10.251.30.10, state=ACTIVE, load=22} NodeStatus{ip=10.251.30.11, state=ACTIVE, load=19} NodeStatus{ip=10.251.30.12, state=ACTIVE, load=17} NodeStatus{ip=10.251.30.13, state=DECOMMISSIONED, load=0} NodeStatus{ip=10.251.30.14, state=ACTIVE, load=21} NodeStatus{ip=10.251.30.15, state=ACTIVE, load=23} NodeStatus{ip=10.251.30.16, state=ACTIVE, load=16} NodeStatus{ip=10.251.30.17, state=ACTIVE, load=18} NodeStatus{ip=10.251.30.18, state=STALE, load=11} NodeStatus{ip=10.251.30.19, state=ACTIVE, load=20} NodeStatus{ip=10.251.30.20, state=ACTIVE, load=24} NodeStatus{ip=10.251.30.21, state=ACTIVE, load=15} NodeStatus{ip=10.251.30.22, state=ACTIVE, load=19} NodeStatus{ip=10.251.30.23, state=ACTIVE, load=22} NodeStatus{ip=10.251.30.24, state=ACTIVE, load=18} NodeStatus{ip=10.251.30.25, state=ACTIVE, load=21} NodeStatus{ip=10.251.30.26, state=ACTIVE, load=17} NodeStatus{ip=10.251.30.27, state=ACTIVE, load=20} NodeStatus{ip=10.251.30.28, state=ACTIVE, load=23} END_DETAIL",
        8.0,
    ),
    (
        "Received block blk_3587508140051953248 of size 67108864 from /10.251.42.84. DataNode: dn9.hadoop.local:50010. Block pool ID: BP-12345-10.0.0.1-1600000000. Namespace ID: 98765. Cluster ID: CID-abc-123. Block report ID: report-555. Storage ID: DS-storage-uuid-999. Finalized length: 67108864. Generation stamp: 1005. State: FINALIZED. Received time: 2025-03-31T18:25:10Z. Write duration: 5500ms. Sync duration: 50ms. Checksum validation: PASSED. ... DETAILS: { ValidationType: FULL, ChecksumAlgorithm: CRC32C, ChunksVerified: 1024/1024, ChunkSize: 65536, ErrorsDetected: 0, TimeTakenMs: 45, SourcePeer: /10.251.42.84:50010, DestinationStorage: { Path: /data1/hdfs/dn/current/BP-12345/current/finalized/subdir1/subdir5/blk_3587508140051953248, StorageType: DISK, CapacityBytes: 10995116277760, UsedBytes: 8265116277760, AvailableBytes: 2730000000000 }, BlockMetadata: { BlockId: blk_3587508140051953248, GenStamp: 1005, NumBytes: 67108864, IsPinned: false, IsCached: false }, PerformanceMetrics: { ReadThroughputMBs: 120.5, WriteThroughputMBs: 115.2, CpuUserPercent: 15.2, CpuSystemPercent: 5.1, IoWaitPercent: 2.5 }, ReplicationInfo: { SourceNodes: [/10.251.42.84], TargetNodes: [self=dn9.hadoop.local:50010], ExpectedReplicas: 3, CurrentReplicasOnNode: 1 }, SecurityContext: { KerberosPrincipal: hdfs/dn9.hadoop.local@EXAMPLE.COM, BlockTokenVerified: true }, OperationId: op_recv_blk_556677 } ... [Simulated Detail Block Repeated For Length - Shortened] Chunk{index=0, offset=0, len=65536, checksum=0x1234abcd, status=OK} Chunk{index=1, offset=65536, len=65536, checksum=0x2345bcde, status=OK} Chunk{index=2, offset=131072, len=65536, checksum=0x3456cdef, status=OK} Chunk{index=3, offset=196608, len=65536, checksum=0x4567def0, status=OK} Chunk{index=4, offset=262144, len=65536, checksum=0x5678ef01, status=OK} Chunk{index=5, offset=327680, len=65536, checksum=0x6789f012, status=OK} Chunk{index=6, offset=393216, len=65536, checksum=0x78900123, status=OK} Chunk{index=7, offset=458752, len=65536, checksum=0x89011234, status=OK} Chunk{index=8, offset=524288, len=65536, checksum=0x90122345, status=OK} Chunk{index=9, offset=589824, len=65536, checksum=0x01233456, status=OK} Chunk{index=10, offset=655360, len=65536, checksum=0x12344567, status=OK} Chunk{index=11, offset=720896, len=65536, checksum=0x23455678, status=OK} Chunk{index=12, offset=786432, len=65536, checksum=0x34566789, status=OK} Chunk{index=13, offset=851968, len=65536, checksum=0x45677890, status=OK} Chunk{index=14, offset=917504, len=65536, checksum=0x56788901, status=OK} Chunk{index=15, offset=983040, len=65536, checksum=0x67899012, status=OK} Chunk{index=16, offset=1048576, len=65536, checksum=0x7890a123, status=OK} Chunk{index=17, offset=1114112, len=65536, checksum=0x8901b234, status=OK} Chunk{index=18, offset=1179648, len=65536, checksum=0x9012c345, status=OK} Chunk{index=19, offset=1245184, len=65536, checksum=0x0123d456, status=OK} Chunk{index=20, offset=1310720, len=65536, checksum=0x1234e567, status=OK} Chunk{index=21, offset=1376256, len=65536, checksum=0x2345f678, status=OK} END_DETAIL",
        5.0,
    ),
    (
        "BLOCK NameSystem.allocateBlock: /user/alice/data/input_file_large.dat. blk_9988776655443322110. Client: DFSClient_NONMAPREDUCE_client-abc_1. Source node: /192.168.10.5. Exclude nodes: [dn10.hadoop.local, dn11.hadoop.local]. Requested size: 134217728. Replication factor: 3. Storage policy: COLD. Available nodes: 48. Chosen nodes: [dn15.hadoop.local:50010, dn21.hadoop.local:50010, dn33.hadoop.local:50010]. Block placement policy: RackAwarePolicy (Default). ... DETAILS: { PlacementPolicyDetails: { NetworkTopology: /datacenter1/rack1/dn15, /datacenter1/rack2/dn21, /datacenter2/rack3/dn33, ExcludedNodesCount: 2, CandidateNodesConsidered: 48, NodesPerRack: { rack1: 10, rack2: 15, rack3: 12, rack4: 11 }, RacksChosen: 3, NodesChosenPerRack: { rack1: 1, rack2: 1, rack3: 1 }, PlacementTimeMs: 12 }, BlockToken: { ExpiryTime: 1679491200000, UserId: alice, BlockPoolId: BP-12345-10.0.0.1-1600000000, AccessModes: [WRITE] }, ClientCapabilities: { HedgedRead: true, ShortCircuitRead: false, ChecksumOffload: true }, NamenodeState: { TotalBlocks: 10.1M, TotalFiles: 5.05M, CapacityUsed: 150.5TB, CapacityRemaining: 55.2TB, ActiveDatanodes: 48, DeadDatanodes: 1, DecommissioningDatanodes: 1 }, OperationMetrics: { LockWaitTimeMs: 1, ProcessingTimeMs: 7, EditLogWriteTimeMs: 2, JournalSyncTimeMs: 3, TotalRpcTimeMs: 13 }, FileContext: { ParentInode: 18000, FileInode: 18555, User: alice, Group: users, Permissions: 0664, IsEncrypted: false, IsErasureCoded: false, StoragePolicy: COLD } } ... [Simulated Detail Block Repeated For Length - Shortened] NodeSelectionAttempt{rack=/rack1, nodes=[dn15, dn16, dn17], chosen=dn15, reason=AVAILABLE_SPACE} NodeSelectionAttempt{rack=/rack2, nodes=[dn21, dn22, dn23, dn24], chosen=dn21, reason=AVAILABLE_SPACE_AND_LOAD} NodeSelectionAttempt{rack=/rack3, nodes=[dn33, dn34], chosen=dn33, reason=NETWORK_DISTANCE} NodeSelectionAttempt{rack=/rack4, nodes=[dn40, dn41, dn42], chosen=null, reason=EXCLUDED_RACK} NodeSelectionAttempt{rack=/rack1, nodes=[dn15, dn16, dn17], chosen=dn16, reason=AVAILABLE_SPACE} NodeSelectionAttempt{rack=/rack2, nodes=[dn21, dn22, dn23, dn24], chosen=dn22, reason=AVAILABLE_SPACE_AND_LOAD} NodeSelectionAttempt{rack=/rack3, nodes=[dn33, dn34], chosen=dn34, reason=NETWORK_DISTANCE} NodeSelectionAttempt{rack=/rack4, nodes=[dn40, dn41, dn42], chosen=null, reason=EXCLUDED_RACK} NodeSelectionAttempt{rack=/rack1, nodes=[dn15, dn16, dn17], chosen=dn17, reason=AVAILABLE_SPACE} NodeSelectionAttempt{rack=/rack2, nodes=[dn21, dn22, dn23, dn24], chosen=dn23, reason=AVAILABLE_SPACE_AND_LOAD} NodeSelectionAttempt{rack=/rack3, nodes=[dn33, dn34], chosen=dn33, reason=NETWORK_DISTANCE} NodeSelectionAttempt{rack=/rack4, nodes=[dn40, dn41, dn42], chosen=null, reason=EXCLUDED_RACK} NodeSelectionAttempt{rack=/rack1, nodes=[dn15, dn16, dn17], chosen=dn15, reason=AVAILABLE_SPACE} NodeSelectionAttempt{rack=/rack2, nodes=[dn21, dn22, dn23, dn24], chosen=dn24, reason=AVAILABLE_SPACE_AND_LOAD} NodeSelectionAttempt{rack=/rack3, nodes=[dn33, dn34], chosen=dn34, reason=NETWORK_DISTANCE} END_DETAIL",
        1.0,
    ),
    (
        "Verification succeeded for blk_-1547954353065580372. Scanner: DataBlockScanner. Scan type: FULL. Duration: 150ms. Block size: 67108864. Storage ID: DS-storage-uuid-123. Volume: /data/hdfs/dn/current. Block pool ID: BP-12345-10.0.0.1-1600000000. Checksum type: CRC32C. Checksum status: OK. Data integrity: VERIFIED. Last verification time: 2025-03-31T18:15:00Z. Next verification scheduled: 2025-04-21T18:15:00Z (3 weeks). ... DETAILS: { ScannerThread: BlockScannerThread-3, CycleStartTime: 1678884000000, CycleEndTime: 1678884150000, BlocksProcessed: 5000, BytesProcessed: 335544320000, VerificationRateBytesSec: 2236962133, VerificationRateBlocksSec: 33, FailedBlocks: [blk_8888, blk_9999], FailedReason: { blk_8888: CHECKSUM_MISMATCH, blk_9999: IO_ERROR }, ReportedToNamenode: true, ReportStatus: SUCCESS, CorrectedErrors: 0, VolumeStats: { Capacity: 10TB, Used: 8TB, Free: 2TB, Health: OK }, DatanodeHealth: { LoadAvg: 0.8, ActiveConnections: 10, HeapUsage: 1.2GB/4GB }, Configuration: { ScanIntensity: 0.1, MaxErrorsPerCycle: 10, ReportIntervalSec: 3600 }, Performance: { AvgBlockScanMs: 30, MaxBlockScanMs: 500, AvgIOReadMBs: 1.0, MaxIOReadMBs: 5.0 } } ... [Simulated Detail Block Repeated For Length - Shortened] ScanResult{blockId=blk_-1547954353065580372, status=OK, durationMs=150, checksum=OK} ScanResult{blockId=blk_-1547954353065580373, status=OK, durationMs=145, checksum=OK} ScanResult{blockId=blk_-1547954353065580374, status=OK, durationMs=155, checksum=OK} ScanResult{blockId=blk_8888, status=FAILED, durationMs=500, checksum=MISMATCH} ScanResult{blockId=blk_-1547954353065580376, status=OK, durationMs=148, checksum=OK} ScanResult{blockId=blk_-1547954353065580377, status=OK, durationMs=152, checksum=OK} ScanResult{blockId=blk_-1547954353065580378, status=OK, durationMs=149, checksum=OK} ScanResult{blockId=blk_9999, status=FAILED, durationMs=450, checksum=IO_ERROR} ScanResult{blockId=blk_-1547954353065580380, status=OK, durationMs=151, checksum=OK} ScanResult{blockId=blk_-1547954353065580381, status=OK, durationMs=147, checksum=OK} ScanResult{blockId=blk_-1547954353065580382, status=OK, durationMs=153, checksum=OK} ScanResult{blockId=blk_-1547954353065580383, status=OK, durationMs=150, checksum=OK} ScanResult{blockId=blk_-1547954353065580384, status=OK, durationMs=146, checksum=OK} ScanResult{blockId=blk_-1547954353065580385, status=OK, durationMs=154, checksum=OK} ScanResult{blockId=blk_-1547954353065580386, status=OK, durationMs=149, checksum=OK} ScanResult{blockId=blk_-1547954353065580387, status=OK, durationMs=151, checksum=OK} ScanResult{blockId=blk_-1547954353065580388, status=OK, durationMs=148, checksum=OK} ScanResult{blockId=blk_-1547954353065580389, status=OK, durationMs=152, checksum=OK} ScanResult{blockId=blk_-1547954353065580390, status=OK, durationMs=150, checksum=OK} ScanResult{blockId=blk_-1547954353065580391, status=OK, durationMs=147, checksum=OK} ScanResult{blockId=blk_-1547954353065580392, status=OK, durationMs=153, checksum=OK} ScanResult{blockId=blk_-1547954353065580393, status=OK, durationMs=149, checksum=OK} END_DETAIL",
        3.0,
    ),
    (
        "PacketResponder 0 for block blk_-6952295868487656571 terminating. Client: /10.251.100.99:55123. Target datanodes: [dn44.hadoop.local:50010, dn45.hadoop.local:50010]. Bytes written: 33554432. Packets acknowledged: 512. Last sequence number: 1024. Status: SUCCESS. Reason: End of block stream received from client. Duration: 3500ms. Average packet processing time: 2ms. Max packet queue size: 10. Current queue size: 0. Sync operations performed: 1. ... DETAILS: { ResponderId: 0, BlockId: blk_-6952295868487656571, GenStamp: 1050, ClientAddr: /10.251.100.99:55123, PipelineTargets: [dn44:50010, dn45:50010], FinalSizeBytes: 33554432, FinalSeqNo: 1024, TerminationReason: EOS_RECEIVED, TotalDurationMs: 3500, AvgPacketProcessMs: 2, MaxPacketQueue: 10, SyncCount: 1, SyncDurationMs: 45, ThreadName: PacketResponderThread-0, ReceiverState: CLOSED, PipelineAckStatus: FULLY_ACKED, Resources: { BuffersFreed: 16, ThreadPoolReturned: true }, FinalizationState: COMMIT_PENDING, DataXceiverThread: DataXceiverThread-8, BlockFile: { FD: 55, Path: /data2/hdfs/dn/current/BP-12345/current/rbw/blk_-6952295868487656571, State: CLOSED }, MetaFile: { FD: 56, Path: /data2/hdfs/dn/current/BP-12345/current/rbw/blk_-6952295868487656571_1050.meta, State: CLOSED }, Perf: { DiskWriteMB: 32, NetSentAckMB: 1, NetRecvDataMB: 32, CpuTimeMs: 150 }, DatanodeStats: { ActiveResponders: 7, MaxResponders: 10 } } ... [Simulated Detail Block Repeated For Length - Shortened] PacketAck{seq=1, status=OK, downstream=dn44} PacketAck{seq=2, status=OK, downstream=dn44} PacketAck{seq=3, status=OK, downstream=dn44} PacketAck{seq=4, status=OK, downstream=dn44} PacketAck{seq=5, status=OK, downstream=dn44} PacketAck{seq=6, status=OK, downstream=dn44} PacketAck{seq=7, status=OK, downstream=dn44} PacketAck{seq=8, status=OK, downstream=dn44} PacketAck{seq=9, status=OK, downstream=dn44} PacketAck{seq=10, status=OK, downstream=dn44} PacketAck{seq=11, status=OK, downstream=dn44} PacketAck{seq=12, status=OK, downstream=dn44} PacketAck{seq=13, status=OK, downstream=dn44} PacketAck{seq=14, status=OK, downstream=dn44} PacketAck{seq=15, status=OK, downstream=dn44} PacketAck{seq=16, status=OK, downstream=dn44} PacketAck{seq=17, status=OK, downstream=dn44} PacketAck{seq=18, status=OK, downstream=dn44} PacketAck{seq=19, status=OK, downstream=dn44} PacketAck{seq=20, status=OK, downstream=dn44} PacketAck{seq=21, status=OK, downstream=dn44} PacketAck{seq=22, status=OK, downstream=dn44} PacketAck{seq=23, status=OK, downstream=dn44} PacketAck{seq=24, status=OK, downstream=dn44} PacketAck{seq=25, status=OK, downstream=dn44} PacketAck{seq=26, status=OK, downstream=dn44} PacketAck{seq=27, status=OK, downstream=dn44} PacketAck{seq=28, status=OK, downstream=dn44} PacketAck{seq=29, status=OK, downstream=dn44} PacketAck{seq=30, status=OK, downstream=dn44} PacketAck{seq=31, status=OK, downstream=dn44} PacketAck{seq=32, status=OK, downstream=dn44} PacketAck{seq=33, status=OK, downstream=dn44} PacketAck{seq=34, status=OK, downstream=dn44} PacketAck{seq=35, status=OK, downstream=dn44} END_DETAIL",
        0.5,
    ),
    (
        "BLOCK ask 10.250.18.114:50010 to delete blk_-5140072410813878235. Reason: Received delete hint from NameNode. Block pool ID: BP-67890-10.0.0.2-1610000000. Deletion task ID: task-del-500. Priority: NORMAL. Block size: 67108864. Generation stamp: 1015. File path on volume: /data/hdfs/dn/current/BP-67890-10.0.0.2-1610000000/current/finalized/subdir8/subdir3/blk_-5140072410813878235. ... DETAILS: { DeletionSource: NameNodeHint(txId=600500, handler=12), BlockInfo: { BlockId: blk_-5140072410813878235, GenStamp: 1015, SizeBytes: 67108864, PoolId: BP-67890-10.0.0.2-1610000000 }, StorageInfo: { StorageId: DS-storage-uuid-abc, VolumePath: /data/hdfs/dn/current, BlockFilePath: ..., MetaFilePath: ... }, TaskInfo: { TaskId: task-del-500, Priority: NORMAL, ScheduledTime: 1678886420000, ExecutionThread: DeletionTaskThread-1 }, PreCheck: { FilesystemReachable: true, BlockFileExists: true, MetaFileExists: true }, Execution: { UnlinkBlockStatus: PENDING, UnlinkMetaStatus: PENDING, SpaceReclaimedEstimateMB: 65 }, PostCheck: { VolumeMapUpdateNeeded: true, ReportToNamenodeNeeded: true }, DatanodeState: { Active: true, StorageReportsPending: 1, LastHeartbeat: 5s ago }, NamenodeConnection: { Host: nn1.hadoop.local:8020, State: CONNECTED } } ... [Simulated Detail Block Repeated For Length - Shortened] DeleteTask{block=blk_-5140072410813878235, reason=NN_HINT, prio=NORMAL} DeleteTask{block=blk_-5140072410813878236, reason=REPL_ADJUST, prio=LOW} DeleteTask{block=blk_-5140072410813878237, reason=USER_REQUEST, prio=HIGH} DeleteTask{block=blk_-5140072410813878238, reason=NN_HINT, prio=NORMAL} DeleteTask{block=blk_-5140072410813878239, reason=REPL_ADJUST, prio=LOW} DeleteTask{block=blk_-5140072410813878240, reason=USER_REQUEST, prio=HIGH} DeleteTask{block=blk_-5140072410813878241, reason=NN_HINT, prio=NORMAL} DeleteTask{block=blk_-5140072410813878242, reason=REPL_ADJUST, prio=LOW} DeleteTask{block=blk_-5140072410813878243, reason=USER_REQUEST, prio=HIGH} DeleteTask{block=blk_-5140072410813878244, reason=NN_HINT, prio=NORMAL} DeleteTask{block=blk_-5140072410813878245, reason=REPL_ADJUST, prio=LOW} DeleteTask{block=blk_-5140072410813878246, reason=USER_REQUEST, prio=HIGH} DeleteTask{block=blk_-5140072410813878247, reason=NN_HINT, prio=NORMAL} DeleteTask{block=blk_-5140072410813878248, reason=REPL_ADJUST, prio=LOW} DeleteTask{block=blk_-5140072410813878249, reason=USER_REQUEST, prio=HIGH} DeleteTask{block=blk_-5140072410813878250, reason=NN_HINT, prio=NORMAL} DeleteTask{block=blk_-5140072410813878251, reason=REPL_ADJUST, prio=LOW} DeleteTask{block=blk_-5140072410813878252, reason=USER_REQUEST, prio=HIGH} DeleteTask{block=blk_-5140072410813878253, reason=NN_HINT, prio=NORMAL} DeleteTask{block=blk_-5140072410813878254, reason=REPL_ADJUST, prio=LOW} DeleteTask{block=blk_-5140072410813878255, reason=USER_REQUEST, prio=HIGH} DeleteTask{block=blk_-5140072410813878256, reason=NN_HINT, prio=NORMAL} DeleteTask{block=blk_-5140072410813878257, reason=REPL_ADJUST, prio=LOW} DeleteTask{block=blk_-5140072410813878258, reason=USER_REQUEST, prio=HIGH} END_DETAIL",
        1.0,
    ),
    (
        "10.251.194.213:50010 Served block blk_-7724713468912166542 to /10.251.203.80. Client request type: READ_BLOCK. Offset: 0. Length: 67108864. Max length: 67108864. Send checksums: true. Caching strategy: default. Block pool ID: BP-12345-10.0.0.1-1600000000. Block token validation: SUCCESS. Token expiry: 2025-04-01T10:00:00Z. Read duration: 1200ms. Transfer rate: 55.9 MB/s. ... DETAILS: { ReadOperationId: op_read_blk_889900, ClientAddr: /10.251.203.80, BlockId: blk_-7724713468912166542, GenStamp: 1099, RequestedOffset: 0, RequestedLen: 67108864, ActualLenSent: 67108864, SendChecksums: true, CachingStrategy: { DropBehind: null, Readahead: null }, BlockToken: { UserId: client_user, Expiry: ..., AccessModes: [READ] }, Performance: { DurationMs: 1200, TransferRateMBs: 55.9, DiskReadBytes: 67108864, DiskReadTimeMs: 1100, NetworkWriteBytes: 67108864, NetworkWriteTimeMs: 1150, ZeroCopy: true, CpuTimeMs: 80 }, StorageInfo: { Path: /data3/hdfs/dn/current/BP-12345/current/finalized/subdir2/subdir9/blk_-7724713468912166542, FD: 68, StorageType: DISK }, DatanodeState: { Load: 25, ActiveConnections: 15, HeapUsage: 2.1GB/8GB }, BlockState: { IsPinned: false, IsCached: true, CachePool: default_pool, CacheHits: 150, CacheMisses: 850 }, ChecksumInfo: { Type: CRC32C, BytesPerChecksum: 512, Verification: ON_READ, ErrorsFound: 0 }, ConnectionInfo: { Protocol: TCP, SocketOpts: { SO_RCVBUF: 262144, SO_SNDBUF: 262144 }, State: CLOSED_WAIT } } ... [Simulated Detail Block Repeated For Length - Shortened] ReadChunk{offset=0, len=65536, status=OK, cache=HIT} ReadChunk{offset=65536, len=65536, status=OK, cache=HIT} ReadChunk{offset=131072, len=65536, status=OK, cache=MISS} ReadChunk{offset=196608, len=65536, status=OK, cache=MISS} ReadChunk{offset=262144, len=65536, status=OK, cache=HIT} ReadChunk{offset=327680, len=65536, status=OK, cache=HIT} ReadChunk{offset=393216, len=65536, status=OK, cache=HIT} ReadChunk{offset=458752, len=65536, status=OK, cache=MISS} ReadChunk{offset=524288, len=65536, status=OK, cache=HIT} ReadChunk{offset=589824, len=65536, status=OK, cache=HIT} ReadChunk{offset=655360, len=65536, status=OK, cache=MISS} ReadChunk{offset=720896, len=65536, status=OK, cache=MISS} ReadChunk{offset=786432, len=65536, status=OK, cache=HIT} ReadChunk{offset=851968, len=65536, status=OK, cache=HIT} ReadChunk{offset=917504, len=65536, status=OK, cache=HIT} ReadChunk{offset=983040, len=65536, status=OK, cache=MISS} ReadChunk{offset=1048576, len=65536, status=OK, cache=HIT} ReadChunk{offset=1114112, len=65536, status=OK, cache=HIT} ReadChunk{offset=1179648, len=65536, status=OK, cache=MISS} ReadChunk{offset=1245184, len=65536, status=OK, cache=MISS} ReadChunk{offset=1310720, len=65536, status=OK, cache=HIT} ReadChunk{offset=1376256, len=65536, status=OK, cache=HIT} ReadChunk{offset=1441792, len=65536, status=OK, cache=HIT} ReadChunk{offset=1507328, len=65536, status=OK, cache=MISS} ReadChunk{offset=1572864, len=65536, status=OK, cache=HIT} ReadChunk{offset=1638400, len=65536, status=OK, cache=HIT} ReadChunk{offset=1703936, len=65536, status=OK, cache=MISS} END_DETAIL",
        1.0,
    ),
    (
        "Deleting block blk_1781953582842324563 file /mnt/hadoop/dfs/data/current/subdir5/blk_1781953582842324563. Block pool ID: BP-67890-10.0.0.2-1610000000. Reason: Received deletion command from NameNode (user request or replication adjustment). Deletion task ID: task-del-650. Block size: 67108864. Generation stamp: 1088. Volume: /mnt/hadoop/dfs/data. Storage ID: DS-storage-uuid-789. Deletion initiated by: BlockDeletionTask-2. ... DETAILS: { DeletionTimestamp: 1678886430000, BlockId: blk_1781953582842324563, GenStamp: 1088, PoolId: BP-67890-10.0.0.2-1610000000, ReasonCode: NN_COMMAND, Source: NameNode(txId=600800), TaskId: task-del-650, VolumeInfo: { Path: /mnt/hadoop/dfs/data, StorageId: DS-storage-uuid-789, Type: DISK }, FilePaths: { Block: /mnt/hadoop/dfs/data/current/subdir5/blk_1781953582842324563, Meta: /mnt/hadoop/dfs/data/current/subdir5/blk_1781953582842324563_1088.meta }, ExecutionInfo: { Thread: BlockDeletionTask-2, StartTime: 1678886430005, EndTime: 1678886430010, DurationMs: 5, UnlinkBlockStatus: SUCCESS, UnlinkMetaStatus: SUCCESS, SpaceReclaimedBytes: 68157440 }, ReportingInfo: { ReportToNamenode: true, RPCMethod: blockReceivedAndDeleted, Status: SUCCESS, ResponseTimeMs: 8 }, VolumeStateBefore: { BlockCount: 1.8M, SpaceUsed: 120.5TB }, VolumeStateAfter: { BlockCount: 1.799999M, SpaceUsed: 120.435TB }, QueueState: { PendingDeletes: 0, ActiveThreads: 1, MaxThreads: 5 } } ... [Simulated Detail Block Repeated For Length - Shortened] DeletionReport{block=blk_1781953582842324563, status=SUCCESS, timeMs=5} DeletionReport{block=blk_1781953582842324564, status=PENDING, timeMs=0} DeletionReport{block=blk_1781953582842324565, status=PENDING, timeMs=0} DeletionReport{block=blk_1781953582842324566, status=SUCCESS, timeMs=4} DeletionReport{block=blk_1781953582842324567, status=PENDING, timeMs=0} DeletionReport{block=blk_1781953582842324568, status=PENDING, timeMs=0} DeletionReport{block=blk_1781953582842324569, status=SUCCESS, timeMs=6} DeletionReport{block=blk_1781953582842324570, status=PENDING, timeMs=0} DeletionReport{block=blk_1781953582842324571, status=PENDING, timeMs=0} DeletionReport{block=blk_1781953582842324572, status=SUCCESS, timeMs=5} DeletionReport{block=blk_1781953582842324573, status=PENDING, timeMs=0} DeletionReport{block=blk_1781953582842324574, status=PENDING, timeMs=0} DeletionReport{block=blk_1781953582842324575, status=SUCCESS, timeMs=4} DeletionReport{block=blk_1781953582842324576, status=PENDING, timeMs=0} DeletionReport{block=blk_1781953582842324577, status=PENDING, timeMs=0} DeletionReport{block=blk_1781953582842324578, status=SUCCESS, timeMs=6} DeletionReport{block=blk_1781953582842324579, status=PENDING, timeMs=0} DeletionReport{block=blk_1781953582842324580, status=PENDING, timeMs=0} DeletionReport{block=blk_1781953582842324581, status=SUCCESS, timeMs=5} DeletionReport{block=blk_1781953582842324582, status=PENDING, timeMs=0} DeletionReport{block=blk_1781953582842324583, status=PENDING, timeMs=0} DeletionReport{block=blk_1781953582842324584, status=SUCCESS, timeMs=4} END_DETAIL",
        1.0,
    ),
];

// From: https://github.com/logpai/loghub-2.0/blob/main/2k_dataset/Spark/Spark_2k.log_structured_corrected.csv
const SPARK_COMPONENTS: [(&str, f32); 3] = [
    ("executor.CoarseGrainedExecutorBackend", 6.0),
    ("spark.SecurityManager", 3.0),
    ("Remoting", 0.5),
];

// From: https://github.com/logpai/loghub-2.0/blob/main/2k_dataset/Spark/Spark_2k.log_structured_corrected.csv
// Log lines of ~3kb to aid cost estimation workflow
const SPARK_MESSAGES: [(&str, f32); 10] = [
    (
        "Created local directory at /opt/spark/work/app-20250331154700-0001/executor-1/blockmgr-uuid-1. ... DETAILS: { ExecutorID: 1, ApplicationID: app-20250331154700-0001, HostPort: worker-1.spark.local:40123, Cores: 4, Memory: 8192M, WorkDir: /opt/spark/work/app-20250331154700-0001/executor-1, LogUrls: {'stdout': 'http://worker-1:8081/logPage?appId=app-20250331154700-0001&executorId=1&logType=stdout', 'stderr': 'http://worker-1:8081/logPage?appId=app-20250331154700-0001&executorId=1&logType=stderr'}, BlockManagerId: { ExecutorID: 1, Host: worker-1.spark.local, Port: 38123 }, Configuration: {'spark.executor.memory': '8g', 'spark.driver.cores': '2', 'spark.app.name': 'DataProcessingJob', 'spark.master': 'spark://master.spark.local:7077', 'spark.shuffle.service.enabled': 'true', 'spark.dynamicAllocation.enabled': 'true', 'spark.dynamicAllocation.minExecutors': '2', 'spark.dynamicAllocation.maxExecutors': '50', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer'}, Resources: {'CPU': 4, 'Memory': 8192, 'GPU': 0}, JvmInfo: { JavaVersion: '11.0.18', VmName: 'OpenJDK 64-Bit Server VM', VmVendor: 'Eclipse Adoptium', MaxHeapSizeMb: 7372 }, EnvironmentVars: {'SPARK_EXECUTOR_CORES': '4', 'SPARK_EXECUTOR_MEMORY': '8g', 'SPARK_APPLICATION_ID': 'app-20250331154700-0001', 'SPARK_EXECUTOR_ID': '1'}, ClasspathEntries: ['/opt/spark/jars/spark-core_2.12-3.4.0.jar', '/opt/spark/jars/scala-library-2.12.15.jar', '/app/libs/my-custom-udf.jar', ... (150 more entries)] } ... [Simulated Detail Repeated For Length] TaskInfo{taskId=100, stageId=5, attempt=0, status=RUNNING, launchTime=1678888021000} TaskInfo{taskId=101, stageId=5, attempt=0, status=RUNNING, launchTime=1678888021010} TaskInfo{taskId=102, stageId=5, attempt=0, status=RUNNING, lauhTime=1678888021140} TaskInfo{taskId=115, stageId=5, attempt=0, status=RUNNING, launchTime=1678888021150} TaskInfo{taskId=116, stageId=5, attempt=0, status=RUNNING, launchTime=1678888021160} TaskInfo{taskId=117, stageId=5, attempt=0, status=RUNNING, launchTime=1678888021170} TaskInfo{taskId=118, stageId=5, attempt=0, status=RUNNING, launchTime=1678888021180} TaskInfo{taskId=119, stageId=5, attempt=0, status=RUNNING, launchTime=1678888021190} TaskInfo{taskId=120, stageId=5, attempt=0, status=RUNNING, launchTime=1678888021200} TaskInfo{taskId=121, stageId=5, attempt=0, status=RUNNING, launchTime=1678888021210} TaskInfo{taskId=122, stageId=5, attempt=0, status=RUNNING, launchTime=1678888021220} TaskInfo{taskId=123, stageId=5, attempt=0, status=RUNNING, launchTime=1678888021230} TaskInfo{taskId=124, stageId=5, attempt=0, status=RUNNING, launchTime=1678888021240} TaskInfo{taskId=125, stageId=5, attempt=0, status=RUNNING, launchTime=1678888021250} TaskInfo{taskId=126, stageId=5, attempt=0, status=RUNNING, launchTime=1678888021260} TaskInfo{taskId=127, stageId=5, attempt=0, status=RUNNING, launchTime=1678888021270} TaskInfo{taskId=128, stageId=5, attempt=0, status=RUNNING, launchTime=1678888021280} END_DETAIL",
        1.0,
    ),
    (
        "Authentication disabled; ui acls disabled; users with view permissions: Set(yarn, spark_user); users with modify permissions: Set(yarn, spark_user). ... DETAILS: { SecurityConfig: { spark.authenticate: false, spark.authenticate.secret: null, spark.network.crypto.enabled: false, spark.ui.acls.enable: false, spark.ui.view.acls: Set(yarn, spark_user), spark.modify.acls: Set(yarn, spark_user), spark.admin.acls: Set(), spark.ssl.enabled: false }, KerberosConfig: { Principal: null, Keytab: null }, PermissionChecks: { checkUIViewPermissions: false, checkModifyPermissions: false, checkAdminPermissions: false }, EffectivePermissions: { ViewUsers: [yarn, spark_user], ModifyUsers: [yarn, spark_user], AdminUsers: [] }, ImpersonationEnabled: false, GroupProvider: org.apache.hadoop.security.ShellBasedUnixGroupsMapping, SecretKey: null, InitializationTimeMs: 15 } ... [Simulated Detail Repeated For Length] SecurityCheck{user=test_user, permission=VIEW, result=DENIED} SecurityCheck{user=yarn, permission=VIEW, result=ALLOWED} SecurityCheck{user=spark_user, permission=MODIFY, result=ALLOWED} SecurityCheck{user=admin, permission=ADMIN, result=DENIED} SecurityCheck{user=guest, permission=VIEW, result=DENIED} SecurityCheck{user=test_user, permission=MODIFY, result=DENIED} SecurityCheck{user=yarn, permission=ADMIN, result=DENIED} SecurityCheck{user=spark_user, permission=VIEW, result=ALLOWED} SecurityCheck{user=admin, permission=MODIFY, result=DENIED} SecurityCheck{user=guest, permission=MODIFY, result=DENIED} SecurityCheck{user=test_user, permission=ADMIN, result=DENIED} SecurityCheck{user=yarn, permission=MODIFY, result=ALLOWED} SecurityCheck{user=spark_user, permission=ADMIN, result=DENIED} SecurityCheck{user=admin, permission=VIEW, result=DENIED} SecurityCheck{user=guest, permission=ADMIN, result=DENIED} SecurityCheck{user=test_user, permission=VIEW, result=DENIED} SecurityCheck{user=yarn, permission=VIEW, result=ALLOWED} SecurityCheck{user=spark_user, permission=MODIFY, result=ALLOWED} SecurityCheck{user=admin, permission=ADMIN, result=DENIED} SecurityCheck{user=guest, permission=VIEW, result=DENIED} SecurityCheck{user=test_user, permission=MODIFY, result=DENIED} SecurityCheck{user=yarn, permission=ADMIN, result=DENIED} SecurityCheck{user=spark_user, permission=VIEW, result=ALLOWED} SecurityCheck{user=admin, permission=MODIFY, result=DENIED} SecurityCheck{user=guest, permission=MODIFY, result=DENIED} SecurityCheck{user=test_user, permission=ADMIN, result=DENIED} SecurityCheck{user=yarn, permission=MODIFY, result=ALLOWED} SecurityCheck{user=spark_user, permission=ADMIN, result=DENIED} SecurityCheck{user=admin, permission=VIEW, result=DENIED} SecurityCheck{user=guest, permission=ADMIN, result=DENIED} SecurityCheck{user=test_user, permission=VIEW, result=DENIED} SecurityCheck{user=yarn, permission=VIEW, result=ALLOWED} SecurityCheck{user=spark_user, permission=MODIFY, result=ALLOWED} SecurityCheck{user=admin, permission=ADMIN, result=DENIED} END_DETAIL",
        0.5,
    ),
    (
        "Connecting to driver: spark://CoarseGrainedScheduler@10.10.34.11:48069. ... DETAILS: { DriverUrl: spark://CoarseGrainedScheduler@10.10.34.11:48069, ExecutorID: 2, ApplicationID: app-20250331154700-0002, WorkerUrl: null, UserClassPath: [], IsLocal: false, Hostname: worker-2.spark.local, CoresAssigned: 8, ConnectionAttempts: 1, ConnectionTimeoutMs: 60000, RpcEnvConfig: { ConnectThreads: 4, IoThreads: 8, NetworkTimeout: 120s, AskTimeout: 30s }, SecurityManager: org.apache.spark.SecurityManager@abcdef0, InitialRegistrationStatus: PENDING, DriverAttributes: null, ExecutorState: CONNECTING } ... [Simulated Detail Repeated For Length] ConnectionAttempt{attempt=1, timeMs=10, status=SUCCESS, target='10.10.34.11:48069'} RpcCall{method='RegisterExecutor', args=[executorId=2, hostPort='worker-2:40124', cores=8], status=SENT, timeMs=15} RpcResponse{method='RegisterExecutor', status=RECEIVED, result=SUCCESS(driverAttributes={...}), timeMs=50} ExecutorStateChange{from=CONNECTING, to=REGISTERED, timeMs=55} RpcCall{method='LaunchTask', args=[taskId=200, stageId=10, attempt=0], status=SENT, timeMs=100} RpcResponse{method='LaunchTask', status=RECEIVED, result=ACK, timeMs=110} ExecutorStateChange{from=REGISTERED, to=RUNNING_TASK, timeMs=115} ConnectionAttempt{attempt=1, timeMs=12, status=SUCCESS, target='10.10.34.11:48069'} RpcCall{method='RegisterExecutor', args=[executorId=3, hostPort='worker-3:40125', cores=8], status=SENT, timeMs=18} RpcResponse{method='RegisterExecutor', status=RECEIVED, result=SUCCESS(driverAttributes={...}), timeMs=60} ExecutorStateChange{from=CONNECTING, to=REGISTERED, timeMs=65} RpcCall{method='LaunchTask', args=[taskId=201, stageId=10, attempt=0], status=SENT, timeMs=120} RpcResponse{method='LaunchTask', status=RECEIVED, result=ACK, timeMs=130} ExecutorStateChange{from=REGISTERED, to=RUNNING_TASK, timeMs=135} ConnectionAttempt{attempt=1, timeMs=15, status=SUCCESS, target='10.10.34.11:48069'} RpcCall{method='RegisterExecutor', args=[executorId=4, hostPort='worker-4:40126', cores=8], status=SENT, timeMs=20} RpcResponse{method='RegisterExecutor', status=RECEIVED, result=SUCCESS(driverAttributes={...}), timeMs=70} ExecutorStateChange{from=CONNECTING, to=REGISTERED, timeMs=75} RpcCall{method='LaunchTask', args=[taskId=202, stageId=10, attempt=0], status=SENT, timeMs=140} RpcResponse{method='LaunchTask', status=RECEIVED, result=ACK, timeMs=150} ExecutorStateChange{from=REGISTERED, to=RUNNING_TASK, timeMs=155} ConnectionAttempt{attempt=1, timeMs=11, status=SUCCESS, target='10.10.34.11:48069'} RpcCall{method='RegisterExecutor', args=[executorId=5, hostPort='worker-5:40127', cores=8], status=SENT, timeMs=16} RpcResponse{method='RegisterExecutor', status=RECEIVED, result=SUCCESS(driverAttributes={...}), timeMs=55} ExecutorStateChange{from=CONNECTING, to=REGISTERED, timeMs=60} RpcCall{method='LaunchTask', args=[taskId=203, stageId=10, attempt=0], status=SENT, timeMs=110} RpcResponse{method='LaunchTask', status=RECEIVED, result=ACK, timeMs=120} END_DETAIL",
        0.5,
    ),
    (
        "Registered signal handlers for [TERM, HUP, INT]. ... DETAILS: { Signals: [TERM, HUP, INT], HandlerClass: org.apache.spark.executor.CoarseGrainedExecutorBackend_anon_1, Action: Shutdown Executor, PreviousHandlers: {TERM: default, HUP: default, INT: default}, ExecutorID: 3, ApplicationID: app-20250331154700-0003, ProcessID: 54321, ThreadID: main, Hostname: worker-3.spark.local, RegistrationTime: 1678888024000 } ... [Simulated Detail Repeated For Length] SignalInfo{signal=TERM, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=HUP, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=INT, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=TERM, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=HUP, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=INT, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=TERM, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=HUP, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=INT, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=TERM, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=HUP, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=INT, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=TERM, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=HUP, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=INT, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=TERM, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=HUP, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=INT, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=TERM, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=HUP, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=INT, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=TERM, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=HUP, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=INT, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=TERM, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=HUP, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=INT, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=TERM, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=HUP, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=INT, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=TERM, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=HUP, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=INT, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=TERM, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=HUP, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=INT, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=TERM, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=HUP, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=INT, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=TERM, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=HUP, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=INT, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=TERM, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=HUP, action=SHUTDOWN, handler=anon_1} SignalInfo{signal=INT, action=SHUTDOWN, handler=anon_1} END_DETAIL",
        1.0,
    ),
    (
        "Created local directory at /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0147/blockmgr-70293f72-844a-4b39-9ad6-fb0ad7e364e4 DETAILS: { ServiceName: org.apache.spark.network.netty.NettyBlockTransferService, Hostname: worker-4.spark.local, Port: 40984, MaxRetries: 3, RetryWaitMs: 5000, TransportConf: { Module: network-common, NumThreads: 8, BufferSize: 262144 }, ShuffleServiceEnabled: true, AuthEnabled: false, EncryptionEnabled: false, BlockManagerId: { ExecutorID: 4, Host: worker-4.spark.local, Port: 40984 }, ApplicationID: app-20250331154700-0004 } ... [Simulated Detail Repeated For Length] ServiceState{name='NettyBlockTransferService', state=STARTED, timeMs=1678888026000} ServiceState{name='BlockManager', state=INITIALIZED, timeMs=1678888026010} ServiceState{name='MemoryManager', state=INITIALIZED, timeMs=1678888026015} ServiceState{name='DiskBlockManager', state=INITIALIZED, timeMs=1678888026020} ServiceState{name='NettyRpcEnv', state=STARTED, timeMs=1678888026025} ServiceState{name='ExecutorSource', state=REGISTERED, timeMs=1678888026030} ServiceState{name='NettyBlockTransferService', state=STARTED, timeMs=1678888026000} ServiceState{name='BlockManager', state=INITIALIZED, timeMs=1678888026010} ServiceState{name='MemoryManager', state=INITIALIZED, timeMs=1678888026015} ServiceState{name='DiskBlockManager', state=INITIALIZED, timeMs=1678888026020} ServiceState{name='NettyRpcEnv', state=STARTED, timeMs=1678888026025} ServiceState{name='ExecutorSource', state=REGISTERED, timeMs=1678888026030} ServiceState{name='NettyBlockTransferService', state=STARTED, timeMs=1678888026000} ServiceState{name='BlockManager', state=INITIALIZED, timeMs=1678888026010} ServiceState{name='MemoryManager', state=INITIALIZED, timeMs=1678888026015} ServiceState{name='DiskBlockManager', state=INITIALIZED, timeMs=1678888026020} ServiceState{name='NettyRpcEnv', state=STARTED, timeMs=1678888026025} ServiceState{name='ExecutorSource', state=REGISTERED, timeMs=1678888026030} ServiceState{name='NettyBlockTransferService', state=STARTED, timeMs=1678888026000} ServiceState{name='BlockManager', state=INITIALIZED, timeMs=1678888026010} ServiceState{name='MemoryManager', state=INITIALIZED, timeMs=1678888026015} ServiceState{name='DiskBlockManager', state=INITIALIZED, timeMs=1678888026020} ServiceState{name='NettyRpcEnv', state=STARTED, timeMs=1678888026025} ServiceState{name='ExecutorSource', state=REGISTERED, timeMs=1678888026030} ServiceState{name='NettyBlockTransferService', state=STARTED, timeMs=1678888026000} ServiceState{name='BlockManager', state=INITIALIZED, timeMs=1678888026010} ServiceState{name='MemoryManager', state=INITIALIZED, timeMs=1678888026015} ServiceState{name='DiskBlockManager', state=INITIALIZED, timeMs=1678888026020} ServiceState{name='NettyRpcEnv', state=STARTED, timeMs=1678888026025} ServiceState{name='ExecutorSource', state=REGISTERED, timeMs=1678888026030} ServiceState{name='NettyBlockTransferService', state=STARTED, timeMs=1678888026000} END_DETAIL",
        10.0,
    ),
    (
        "Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40984. DETAILS: { ServerAddress: /10.250.10.5:40984, BoundPort: 40984, TransportConf: { Module: network-common, ConnectTimeout: 60s, IoThreads: 4, ReceiveBufferSize: 1048576, SendBufferSize: 1048576 }, Security: { Enabled: false, SaslEnabled: false }, RpcHandler: org.apache.spark.network.server.TransportRequestHandler@1234abcd, StreamManager: org.apache.spark.network.shuffle.ExternalShuffleBlockResolver@fedcba98, Capabilities: { SupportsChunkedFetch: true, SupportsRpcWithStream: true }, InitializationTimeMs: 55 } ... [Simulated Detail Repeated For Length] ChannelInfo{remoteAddress=/10.250.10.10:51234, state=ACTIVE, type=NIO, creationTime=1678888025100} ChannelInfo{remoteAddress=/10.250.10.11:51235, state=ACTIVE, type=NIO, creationTime=1678888025110} ChannelInfo{remoteAddress=/10.250.10.12:51236, state=ACTIVE, type=NIO, creationTime=1678888025120} ChannelInfo{remoteAddress=/10.250.10.13:51237, state=ACTIVE, type=NIO, creationTime=1678888025130} ChannelInfo{remoteAddress=/10.250.10.14:51238, state=ACTIVE, type=NIO, creationTime=1678888025140} ChannelInfo{remoteAddress=/10.250.10.15:51239, state=ACTIVE, type=NIO, creationTime=1678888025150} ChannelInfo{remoteAddress=/10.250.10.16:51240, state=ACTIVE, type=NIO, creationTime=1678888025160} ChannelInfo{remoteAddress=/10.250.10.17:51241, state=ACTIVE, type=NIO, creationTime=1678888025170} ChannelInfo{remoteAddress=/10.250.10.18:51242, state=ACTIVE, type=NIO, creationTime=1678888025180} ChannelInfo{remoteAddress=/10.250.10.19:51243, state=ACTIVE, type=NIO, creationTime=1678888025190} ChannelInfo{remoteAddress=/10.250.10.20:51244, state=ACTIVE, type=NIO, creationTime=1678888025200} ChannelInfo{remoteAddress=/10.250.10.21:51245, state=ACTIVE, type=NIO, creationTime=1678888025210} ChannelInfo{remoteAddress=/10.250.10.22:51246, state=ACTIVE, type=NIO, creationTime=1678888025220} ChannelInfo{remoteAddress=/10.250.10.23:51247, state=ACTIVE, type=NIO, creationTime=1678888025230} ChannelInfo{remoteAddress=/10.250.10.24:51248, state=ACTIVE, type=NIO, creationTime=1678888025240} ChannelInfo{remoteAddress=/10.250.10.25:51249, state=ACTIVE, type=NIO, creationTime=1678888025250} ChannelInfo{remoteAddress=/10.250.10.26:51250, state=ACTIVE, type=NIO, creationTime=1678888025260} ChannelInfo{remoteAddress=/10.250.10.27:51251, state=ACTIVE, type=NIO, creationTime=1678888025270} ChannelInfo{remoteAddress=/10.250.10.28:51252, state=ACTIVE, type=NIO, creationTime=1678888025280} ChannelInfo{remoteAddress=/10.250.10.29:51253, state=ACTIVE, type=NIO, creationTime=1678888025290} ChannelInfo{remoteAddress=/10.250.10.30:51254, state=ACTIVE, type=NIO, creationTime=1678888025300} ChannelInfo{remoteAddress=/10.250.10.31:51255, state=ACTIVE, type=NIO, creationTime=1678888025310} ChannelInfo{remoteAddress=/10.250.10.32:51256, state=ACTIVE, type=NIO, creationTime=1678888025320} ChannelInfo{remoteAddress=/10.250.10.33:51257, state=ACTIVE, type=NIO, creationTime=1678888025330} ChannelInfo{remoteAddress=/10.250.10.34:51258, state=ACTIVE, type=NIO, creationTime=1678888025340} END_DETAIL",
        6.0,
    ),
    (
        "Changing view acls to: yarn,prod_users,dev_users. ... DETAILS: { OldViewAcls: Set(yarn, spark_user), NewViewAcls: Set(yarn, prod_users, dev_users), OldModifyAcls: Set(yarn, spark_user), NewModifyAcls: Set(yarn, spark_user), ConfigSource: DynamicUpdate via RPC, UpdateInitiator: YarnResourceManager, UpdateReason: ACL sync from cluster policy, SecurityManagerID: org.apache.spark.SecurityManager@fedcba0, Timestamp: 1678888027000 } ... [Simulated Detail Repeated For Length] AclUpdate{type=VIEW, action=ADD, user=prod_users, result=SUCCESS} AclUpdate{type=VIEW, action=ADD, user=dev_users, result=SUCCESS} AclUpdate{type=VIEW, action=REMOVE, user=spark_user, result=SUCCESS} AclUpdate{type=MODIFY, action=NO_CHANGE, user=yarn, result=SUCCESS} AclUpdate{type=MODIFY, action=NO_CHANGE, user=spark_user, result=SUCCESS} AclUpdate{type=VIEW, action=ADD, user=prod_users, result=SUCCESS} AclUpdate{type=VIEW, action=ADD, user=dev_users, result=SUCCESS} AclUpdate{type=VIEW, action=REMOVE, user=spark_user, result=SUCCESS} AclUpdate{type=MODIFY, action=NO_CHANGE, user=yarn, result=SUCCESS} AclUpdate{type=MODIFY, action=NO_CHANGE, user=spark_user, result=SUCCESS} AclUpdate{type=VIEW, action=ADD, user=prod_users, result=SUCCESS} AclUpdate{type=VIEW, action=ADD, user=dev_users, result=SUCCESS} AclUpdate{type=VIEW, action=REMOVE, user=spark_user, result=SUCCESS} AclUpdate{type=MODIFY, action=NO_CHANGE, user=yarn, result=SUCCESS} AclUpdate{type=MODIFY, action=NO_CHANGE, user=spark_user, result=SUCCESS} AclUpdate{type=VIEW, action=ADD, user=prod_users, result=SUCCESS} AclUpdate{type=VIEW, action=ADD, user=dev_users, result=SUCCESS} AclUpdate{type=VIEW, action=REMOVE, user=spark_user, result=SUCCESS} AclUpdate{type=MODIFY, action=NO_CHANGE, user=yarn, result=SUCCESS} AclUpdate{type=MODIFY, action=NO_CHANGE, user=spark_user, result=SUCCESS} AclUpdate{type=VIEW, action=ADD, user=prod_users, result=SUCCESS} AclUpdate{type=VIEW, action=ADD, user=dev_users, result=SUCCESS} AclUpdate{type=VIEW, action=REMOVE, user=spark_user, result=SUCCESS} AclUpdate{type=MODIFY, action=NO_CHANGE, user=yarn, result=SUCCESS} AclUpdate{type=MODIFY, action=NO_CHANGE, user=spark_user, result=SUCCESS} AclUpdate{type=VIEW, action=ADD, user=prod_users, result=SUCCESS} AclUpdate{type=VIEW, action=ADD, user=dev_users, result=SUCCESS} AclUpdate{type=VIEW, action=REMOVE, user=spark_user, result=SUCCESS} AclUpdate{type=MODIFY, action=NO_CHANGE, user=yarn, result=SUCCESS} AclUpdate{type=MODIFY, action=NO_CHANGE, user=spark_user, result=SUCCESS} AclUpdate{type=VIEW, action=ADD, user=prod_users, result=SUCCESS} AclUpdate{type=VIEW, action=ADD, user=dev_users, result=SUCCESS} AclUpdate{type=VIEW, action=REMOVE, user=spark_user, result=SUCCESS} AclUpdate{type=MODIFY, action=NO_CHANGE, user=yarn, result=SUCCESS} AclUpdate{type=MODIFY, action=NO_CHANGE, user=spark_user, result=SUCCESS} AclUpdate{type=VIEW, action=ADD, user=prod_users, result=SUCCESS} AclUpdate{type=VIEW, action=ADD, user=dev_users, result=SUCCESS} END_DETAIL",
        1.0,
    ),
    (
        "Started reading broadcast variable 0. ... DETAILS: { BroadcastID: 0, VariableName: 'config_params', SizeBytes: 1048576, Source: Driver, StorageLevel: MEMORY_AND_DISK_SER, ReplicationNodes: [executor-1, executor-2, executor-3, executor-4, executor-5], ReadMechanism: TorrentBroadcast, BlocksTotal: 16, BlockSize: 65536, BlocksRead: 0, BytesRead: 0, ReadStartTime: 1678895401000, EstimatedReadTimeMs: 500, ExecutorID: 1, ApplicationID: app-20250331235000-0001, Host: worker-1.spark.local } ... [Simulated Detail Repeated For Length] BlockStatus{blockId=broadcast_0_piece0, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece1, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece2, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece3, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece4, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece5, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece6, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece7, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece8, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece9, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece10, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece11, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece12, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece13, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece14, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece15, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece0, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece1, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece2, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece3, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece4, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece5, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece6, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece7, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece8, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece9, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece10, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece11, status=PENDING, size=65536, location=driver} BlockStatus{blockId=broadcast_0_piece12, status=PENDING, size=65536, location=driver} END_DETAIL",
        0.5,
    ),
    (
        "Finished task 1.0 in stage 0.0 (TID 1). 2703 bytes result sent to driver. ... DETAILS: { TaskInfo: { TaskID: 1, Index: 1, Attempt: 0, StageID: 0, StageAttemptID: 0, ExecutorID: '2', Host: 'worker-2.spark.local', Locality: PROCESS_LOCAL, LaunchTime: 1678895401500, FinishTime: 1678895402000, Duration: 500ms, GettingResultTime: 2ms, Status: SUCCESS }, TaskMetrics: { ExecutorDeserializeTime: 10ms, ExecutorDeserializeCpuTime: 5ms, ExecutorRunTime: 450ms, ExecutorCpuTime: 300ms, ResultSize: 2703 bytes, JvmGCTime: 50ms, ResultSerializationTime: 5ms, MemoryBytesSpilled: 0, DiskBytesSpilled: 0, ShuffleReadMetrics: { RemoteBlocksFetched: 0, LocalBlocksFetched: 0, FetchWaitTime: 0ms, RemoteBytesRead: 0, RemoteBytesReadToDisk: 0, LocalBytesRead: 0, TotalBytesRead: 0, TotalRecordsRead: 0 }, ShuffleWriteMetrics: { BytesWritten: 0, WriteTime: 0ns, RecordsWritten: 0 }, InputMetrics: { BytesRead: 134217728, RecordsRead: 1000000 }, OutputMetrics: { BytesWritten: 2703, RecordsWritten: 1 }, PeakExecutionMemory: 512MB, UpdatedBlockStatuses: [] } } ... [Simulated Detail Repeated For Length] TaskTiming{step='Deserialize', time=10ms} TaskTiming{step='Run', time=450ms} TaskTiming{step='SerializeResult', time=5ms} TaskTiming{step='SendResult', time=2ms} GcPhase{name='PS MarkSweep', count=1, time=50ms} GcPhase{name='PS Scavenge', count=5, time=15ms} TaskTiming{step='Deserialize', time=10ms} TaskTiming{step='Run', time=450ms} TaskTiming{step='SerializeResult', time=5ms} TaskTiming{step='SendResult', time=2ms} GcPhase{name='PS MarkSweep', count=1, time=50ms} GcPhase{name='PS Scavenge', count=5, time=15ms} TaskTiming{step='Deserialize', time=10ms} TaskTiming{step='Run', time=450ms} TaskTiming{step='SerializeResult', time=5ms} TaskTiming{step='SendResult', time=2ms} GcPhase{name='PS MarkSweep', count=1, time=50ms} GcPhase{name='PS Scavenge', count=5, time=15ms} TaskTiming{step='Deserialize', time=10ms} TaskTiming{step='Run', time=450ms} TaskTiming{step='SerializeResult', time=5ms} TaskTiming{step='SendResult', time=2ms} GcPhase{name='PS MarkSweep', count=1, time=50ms} GcPhase{name='PS Scavenge', count=5, time=15ms} TaskTiming{step='Deserialize', time=10ms} TaskTiming{step='Run', time=450ms} TaskTiming{step='SerializeResult', time=5ms} TaskTiming{step='SendResult', time=2ms} GcPhase{name='PS MarkSweep', count=1, time=50ms} GcPhase{name='PS Scavenge', count=5, time=15ms} TaskTiming{step='Deserialize', time=10ms} TaskTiming{step='Run', time=450ms} TaskTiming{step='SerializeResult', time=5ms} TaskTiming{step='SendResult', time=2ms} GcPhase{name='PS MarkSweep', count=1, time=50ms} GcPhase{name='PS Scavenge', count=5, time=15ms} TaskTiming{step='Deserialize', time=10ms} TaskTiming{step='Run', time=450ms} TaskTiming{step='SerializeResult', time=5ms} TaskTiming{step='SendResult', time=2ms} GcPhase{name='PS MarkSweep', count=1, time=50ms} GcPhase{name='PS Scavenge', count=5, time=15ms} TaskTiming{step='Deserialize', time=10ms} TaskTiming{step='Run', time=450ms} TaskTiming{step='SerializeResult', time=5ms} END_DETAIL",
        0.5,
    ),
    (
        "MemoryStore started with capacity 17.7 GB (19005477662 bytes). ... DETAILS: { BlockManagerId: { ExecutorID: 'driver', Host: 'driver.spark.local', Port: 35001 }, MaxMemoryBytes: 19005477662, MaxOnHeapMemory: 10737418240 (10.0 GB), MaxOffHeapMemory: 8268059422 (7.7 GB), CurrentMemoryUsed: 0 bytes, CurrentOnHeapMemoryUsed: 0 bytes, CurrentOffHeapMemoryUsed: 0 bytes, StorageLevelDefault: StorageLevel(memory, 1 replicas), UnrollMemoryThreshold: 1048576, EvictionPolicy: LRU, Blocks: {} } ... [Simulated Detail Repeated For Length] MemoryPool{name=CodeHeap 'non-nmethods', type=NON_HEAP, usage={init=2555904, used=1000000, committed=2555904, max=10000000}} MemoryPool{name=Metaspace, type=NON_HEAP, usage={init=0, used=50000000, committed=51000000, max=-1}} MemoryPool{name=CodeHeap 'profiled nmethods', type=NON_HEAP, usage={init=2555904, used=20000000, committed=21000000, max=122908672}} MemoryPool{name=Compressed Class Space, type=NON_HEAP, usage={init=0, used=8000000, committed=8388608, max=1073741824}} MemoryPool{name=G1 Eden Space, type=HEAP, usage={init=268435456, used=1073741824, committed=1073741824, max=...}} MemoryPool{name=G1 Old Gen, type=HEAP, usage={init=1073741824, used=500000000, committed=1073741824, max=...}} MemoryPool{name=G1 Survivor Space, type=HEAP, usage={init=0, used=0, committed=0, max=...}} MemoryPool{name=CodeHeap 'non-nmethods', type=NON_HEAP, usage={init=2555904, used=1000000, committed=2555904, max=10000000}} MemoryPool{name=Metaspace, type=NON_HEAP, usage={init=0, used=50000000, committed=51000000, max=-1}} MemoryPool{name=CodeHeap 'profiled nmethods', type=NON_HEAP, usage={init=2555904, used=20000000, committed=21000000, max=122908672}} MemoryPool{name=Compressed Class Space, type=NON_HEAP, usage={init=0, used=8000000, committed=8388608, max=1073741824}} MemoryPool{name=G1 Eden Space, type=HEAP, usage={init=268435456, used=1073741824, committed=1073741824, max=...}} MemoryPool{name=G1 Old Gen, type=HEAP, usage={init=1073741824, used=500000000, committed=1073741824, max=...}} MemoryPool{name=G1 Survivor Space, type=HEAP, usage={init=0, used=0, committed=0, max=...}} MemoryPool{name=CodeHeap 'non-nmethods', type=NON_HEAP, usage={init=2555904, used=1000000, committed=2555904, max=10000000}} MemoryPool{name=Metaspace, type=NON_HEAP, usage={init=0, used=50000000, committed=51000000, max=-1}} MemoryPool{name=CodeHeap 'profiled nmethods', type=NON_HEAP, usage={init=2555904, used=20000000, committed=21000000, max=122908672}} MemoryPool{name=Compressed Class Space, type=NON_HEAP, usage={init=0, used=8000000, committed=8388608, max=1073741824}} MemoryPool{name=G1 Eden Space, type=HEAP, usage={init=268435456, used=1073741824, committed=1073741824, max=...}} MemoryPool{name=G1 Old Gen, type=HEAP, usage={init=1073741824, used=500000000, committed=1073741824, max=...}} MemoryPool{name=G1 Survivor Space, type=HEAP, usage={init=0, used=0, committed=0, max=...}} END_DETAIL",
        0.5,
    ),
];

pub fn infra_log_line() -> InfraLog {
    InfraLog::new()
}

#[derive(Debug, Serialize)]
pub struct InfraLog {
    app: String,
    host: String,
    level: String,
    line: String,
}

impl InfraLog {
    fn new() -> Self {
        let app = choose_weighted(&APPS);
        let level = choose_weighted(&LOG_LEVELS);
        let source_type = choose_weighted(&LOG_SOURCE_TYPES);

        Self {
            app: app.to_string(),
            host: thread_rng()
                .sample::<Domain, _>(rand::distributions::Standard)
                .to_string(),
            level: level.to_string(),
            line: match source_type {
                "SPARK" => spark_log_line(level),
                "NGINX" => nginx_access_log_line(),
                "SYSLOG" => syslog_5424_log_line(),
                _ => hdfs_log_line(level),
            },
        }
    }
}

macro_rules! hdfs_warning {
    ($server_ip:expr, $block_id:expr, $client_ip:expr) => {
        format!(
            "{}:Got exception while serving blk_{} to /{}:",
            $server_ip, $block_id, $client_ip
        )
    };
}

/**
 * Generates HDFS logs matching the following format:
 * <date:YYMMDD> <time:HHMMSS> <pid> <level> <source_component>: <message>
 */
fn hdfs_log_line(level: &str) -> String {
    let pid = thread_rng().gen_range(1024..65535);
    let message = match level {
        "INFO" => choose_weighted(&HDFS_INFO_MESSAGES).to_string(),
        _ => {
            let server_ip = format!("{}:{}", gen_ipv4(), thread_rng().gen_range(1000..65535));
            let block_id: u64 = thread_rng().sample(rand::distributions::Standard);
            let client_ip = gen_ipv4();
            hdfs_warning!(server_ip, block_id, client_ip)
        }
    };

    format!(
        "{} {} {} {}: {}",
        Utc::now().format(HDFS_LOG_TIME_FORMAT),
        pid,
        level,
        choose_weighted(&HDFS_COMPONENTS),
        message
    )
}

/**
 * Generate spark log line matching the following template:
 * date:MM/DD/YY> <time:HH:MM:SS> <level> <component>: <message>
 */
fn spark_log_line(level: &str) -> String {
    format!(
        "{} {} {}: {}",
        Utc::now().format(SPARK_LOG_TIME_FORMAT),
        level,
        choose_weighted(&SPARK_COMPONENTS),
        choose_weighted(&SPARK_MESSAGES)
    )
}
